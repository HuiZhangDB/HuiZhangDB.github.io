<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png">
  <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"huisblog.cn","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="Why Neural Networks?当遇到特征数量很大(n is large)的复杂非线性问题时，简单的逻辑回归（加入二次项或三次项）不再适用，因为会导致太多的特征量参与计算（不仅仅是\(x_n, x_n^2,x_n^3\)，还有\(x_i{x_j},x_i^2{x_j}\)等二次项和三次项）—— 带来巨大的计算代价和过拟合问题。 神经网络其实很早就出现了(A pretty old al">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络(Neural Networks)">
<meta property="og:url" content="http://huisblog.cn/2017/06/15/neuralnetwork/index.html">
<meta property="og:site_name" content="HuisBlog">
<meta property="og:description" content="Why Neural Networks?当遇到特征数量很大(n is large)的复杂非线性问题时，简单的逻辑回归（加入二次项或三次项）不再适用，因为会导致太多的特征量参与计算（不仅仅是\(x_n, x_n^2,x_n^3\)，还有\(x_i{x_j},x_i^2{x_j}\)等二次项和三次项）—— 带来巨大的计算代价和过拟合问题。 神经网络其实很早就出现了(A pretty old al">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-06-15T16:54:11.000Z">
<meta property="article:modified_time" content="2021-10-21T06:13:05.470Z">
<meta property="article:author" content="Hui Zhang">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://huisblog.cn/2017/06/15/neuralnetwork/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://huisblog.cn/2017/06/15/neuralnetwork/","path":"2017/06/15/neuralnetwork/","title":"神经网络(Neural Networks)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>神经网络(Neural Networks) | HuisBlog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">HuisBlog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-blogs"><a href="/" rel="section"><i class="fa fa-sticky-note fa-fw"></i>Blogs</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-huishome"><a href="http://zhanghui.ac.cn/" rel="noopener" target="_blank"><i class="fa fa-user fa-fw"></i>HuisHome</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Why Neural Networks?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Representation"><span class="nav-number">2.</span> <span class="nav-text">Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Application"><span class="nav-number">3.</span> <span class="nav-text">Application</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97"><span class="nav-number">3.1.</span> <span class="nav-text">二元逻辑运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="nav-number">3.2.</span> <span class="nav-text">多类分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Backprogation"><span class="nav-number">5.</span> <span class="nav-text">反向传播(Backprogation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="nav-number">6.</span> <span class="nav-text">神经网络的使用和训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BB%93%E6%9E%84"><span class="nav-number">6.1.</span> <span class="nav-text">神经网络的结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.2.</span> <span class="nav-text">训练神经网络</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hui Zhang"
      src="/assets/avatar.jpg">
  <p class="site-author-name" itemprop="name">Hui Zhang</p>
  <div class="site-description" itemprop="description">Dream It Possible</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HuiZhangDB" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HuiZhangDB" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:helenzhang2015@foxmail.com" title="E-Mail → mailto:helenzhang2015@foxmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Friends
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://blog.imxcy.com/" title="http:&#x2F;&#x2F;blog.imxcy.com&#x2F;" rel="noopener" target="_blank">C.Y.Xu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://aprilwang.me/" title="https:&#x2F;&#x2F;aprilwang.me&#x2F;" rel="noopener" target="_blank">April</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.hlyue.com/" title="https:&#x2F;&#x2F;blog.hlyue.com&#x2F;" rel="noopener" target="_blank">Richard</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.senorsen.com/links/" title="https:&#x2F;&#x2F;blog.senorsen.com&#x2F;links&#x2F;" rel="noopener" target="_blank">Senorsen</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://fujiaqi.com/" title="http:&#x2F;&#x2F;fujiaqi.com&#x2F;" rel="noopener" target="_blank">Jiaqi Fu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://qusic.me/" title="https:&#x2F;&#x2F;qusic.me&#x2F;" rel="noopener" target="_blank">Qusic</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://imsun.net/" title="https:&#x2F;&#x2F;imsun.net&#x2F;" rel="noopener" target="_blank">Sun</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.sfu.ca/~leeaol/" title="https:&#x2F;&#x2F;www.sfu.ca&#x2F;~leeaol&#x2F;" rel="noopener" target="_blank">Leeleo3x</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://mypage.zju.edu.cn/en/zhangkejun" title="http:&#x2F;&#x2F;mypage.zju.edu.cn&#x2F;en&#x2F;zhangkejun" rel="noopener" target="_blank">Kejun Zhang</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://huisblog.cn/2017/06/15/neuralnetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/avatar.jpg">
      <meta itemprop="name" content="Hui Zhang">
      <meta itemprop="description" content="Dream It Possible">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HuisBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          神经网络(Neural Networks)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-06-15 16:54:11" itemprop="dateCreated datePublished" datetime="2017-06-15T16:54:11+00:00">2017-06-15</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-10-21 06:13:05" itemprop="dateModified" datetime="2021-10-21T06:13:05+00:00">2021-10-21</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NG-ML/" itemprop="url" rel="index"><span itemprop="name">NG_ML</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h2 id="Why-Neural-Networks"><a href="#Why-Neural-Networks" class="headerlink" title="Why Neural Networks?"></a>Why Neural Networks?</h2><p>当遇到<code>特征数量很大(n is large)</code>的<code>复杂非线性问题</code>时，简单的逻辑回归（加入二次项或三次项）不再适用，因为会导致太多的特征量参与计算（不仅仅是\(x_n, x_n^2,x_n^3\)，还有\(x_i{x_j},x_i^2{x_j}\)等二次项和三次项）—— 带来巨大的计算代价和过拟合问题。</p>
<p>神经网络其实很早就出现了(A pretty old algorithm)，始于80年代，90年代有所衰减而现在又火了起来（计算机计算能力的提升）。神经网络模拟人脑神经的计算机制，在解决许多机器学习问题时有很好的效果。</p>
<p>人脑使用一种学习算法来处理无数不同的问题，这使得我们模拟它的计算机制成为可能。人脑甚至可以学习任何传感数据，这让神经网络成为最有可能实现人工智能的方法。</p>
<span id="more"></span>

<h2 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h2><p>最简单的神经网络表达式：</p>
<p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \newline \end{bmatrix}\rightarrow h_\theta(x)$$</p>
<p>这是一个只有2层的神经网络，第1层<code>输入层</code>\(\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\)和第2层<code>输出层</code>\(h_\theta(x)\)。</p>
<p>输入层和输出层之间可以有中间层节点，称为<code>隐藏层(hidden layers)</code>。有一个隐藏层的神经网络可以表示为：</p>
<p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)$$</p>
<p>$$\begin{align}&amp; a_i^{(j)} = \text{“activation” of unit i in layer j} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer j to layer j+1}\end{align}$$</p>
<p>每个激活节点的计算方式：</p>
<p>$$\begin{align} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align}$$</p>
<p>值得注意的是，如果神经网络在第\(j\)层有\(s_j\)个节点，在第\(j+1\)层有\(s_{j+1}\)个节点，那么\(\Theta^{(j)}\)的维度会是\(s_{j+1}\times(s_j +1)\)。</p>
<p>增加隐藏层可以解决更多的复杂非线性问题。</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><h3 id="二元逻辑运算"><a href="#二元逻辑运算" class="headerlink" title="二元逻辑运算"></a>二元逻辑运算</h3><p>神经网络可以实现所有的逻辑门，例如：</p>
<p>$$\begin{align}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align}$$</p>
<h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p>比如将数据分为4类，表达式可以是：</p>
<p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline … \newline x_n\end{bmatrix}\rightarrow\begin{bmatrix}a_0^{(2)} \newline a_1^{(2)} \newline a_2^{(2)} \newline … \newline \end{bmatrix}\rightarrow\begin{bmatrix}a_0^{(3)} \newline a_1^{(3)} \newline a_2^{(3)} \newline …\newline \end{bmatrix}\rightarrow … \rightarrow \begin{bmatrix}h_\Theta(x)_1 \newline h_\Theta(x)_2 \newline h_\Theta(x)_3 \newline h_\Theta(x)_4\end{bmatrix}$$</p>
<p>分别训练4个假设函数，表示每个分类的可能性，取可能性最大的标签为分类结果。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>神经网络的损失函数标准形式：<br>$$\begin{gather} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}<em>k \log ((h_\Theta (x^{(i)}))<em>k) + (1 - y^{(i)}<em>k)\log (1 - (h_\Theta(x^{(i)}))<em>k)\right] + \frac{\lambda}{2m}\sum</em>{l=1}^{L-1} \sum</em>{i=1}^{s_l} \sum</em>{j=1}^{s</em>{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather}$$</p>
<p>$$\begin{align}&amp; L = \text{ total number of layers in the network} \newline&amp; s_l = \text{number of units (not counting bias unit) in layer (l)}\newline&amp; K = \text{number of output units/classes}\end{align}$$</p>
<p>注意，第\(l\)层的\(\Theta^{(l)}\)矩阵大小为\(s_{l+1}\times (s_l + 1)\)，因此正则项\(\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\)是除去截距项(\(\Theta^{(l)}\)第一列)的所有\(\theta\)的平方和。</p>
<p>这个损失函数\(J(\Theta)\)是非凸的，因此使用梯度下降可能只能得到局部最优值。</p>
<h2 id="反向传播-Backprogation"><a href="#反向传播-Backprogation" class="headerlink" title="反向传播(Backprogation)"></a>反向传播(Backprogation)</h2><p><code>反向传播算法</code>是用于计算损失函数偏导\(\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)\)的算法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Given training set &#123;(x(1),y(1))⋯(x(m),y(m))&#125;, set Δ(l)i,j := 0 for all (l,i,j), (hence you end up having a matrix full of zeros)</span><br><span class="line"></span><br><span class="line">For training example t =1 to m:</span><br><span class="line">1. Set a(1) := x(t)</span><br><span class="line">2. Perform forward propagation to compute a(l) for l=2,3,…,L</span><br><span class="line">3. Using y(t), compute delta(L) = a(L)−y(t)</span><br><span class="line">4. Compute delta(L−1),delta(L−2),…,delta(2) using delta(l) = ((Θ(l))&#x27;delta(l+1)) .∗ a(l) .∗ (1−a(l))</span><br><span class="line">5. Delta(l):= Delta(l)+ delta(l+1)(a(l))&#x27;</span><br><span class="line">6. Regularization as the following equation.</span><br></pre></td></tr></table></figure>

<p>$$\begin{align}&amp; D_{i,j}^{(l)} = \frac{1}{m}(\Delta_{i,j}^{(l)} + \lambda \Theta_{i,j}^{(l)}) \quad \text{if (j \neq 0)}\newline&amp;  D_{i,j}^{(l)} = \frac{1}{m}\Delta_{i,j}^{(l)} \quad \text{if (j = 0)}\end{align}$$</p>
<p>最后得到的\(D_{i,j}^{(l)}\)即要求的偏导\(\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)\)。</p>
<p>可以使用<code>梯度检查(Gradient Checking)</code>来检查反向传播算法实现得是否正确。其依据：<br>$$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$$</p>
<p>对于每一个\(\Theta^{(j)}\)即：</p>
<p>$$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$$</p>
<p>当\(\epsilon = 10^{-4} \)时，可以保证其数学意义（足够小），再小的话计算机在计算的时候会出现数值错误，梯度检查的MATLAB实现：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</span><br><span class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon)</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>

<h2 id="神经网络的使用和训练"><a href="#神经网络的使用和训练" class="headerlink" title="神经网络的使用和训练"></a>神经网络的使用和训练</h2><h3 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h3><p>在准备使用神经网络来解决问题时，首先要设计神经网络的结构，包括有多少层隐藏层以及每层有多少节点：</p>
<ol>
<li>输入层节点数，即特征\(x^{(i)}\)的维度；</li>
<li>输出层节点数，即分类的类数；</li>
<li>隐藏层数目和每层的节点数，通常越多越好，但必须平衡随之增加的计算代价；</li>
<li>默认原则：1个隐藏层，如果有1个以上的隐藏层，那么建议每层的节点数一样。</li>
</ol>
<h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ol>
<li>随机生成每层的\(\Theta^{(l)}\)；</li>
<li>使用正向传播算出每个训练数据\(x^{(i)}\)的预测值\(h_{\Theta}(x^{(i)})\)；</li>
<li>实现损失函数；</li>
<li>使用反向传播算法实现损失函数偏导计算；</li>
<li>使用梯度检查确保反向传播算法正确实现；</li>
<li>使用梯度下降或其他优化算法来最小化损失函数，得到最终的\(\Theta^{(l)}\)值。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML/" rel="tag"># ML</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/06/03/logisticreg/" rel="prev" title="逻辑回归(Logistic Regression)">
                  <i class="fa fa-chevron-left"></i> 逻辑回归(Logistic Regression)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2017/06/20/betterMLA/" rel="next" title="提升学习算法性能">
                  提升学习算法性能 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.2.6/pdfobject.min.js","integrity":"sha256-77geM50MfxCD17eqyJR+Dag1svjJOLN+BJ2F/DMqMEY="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>




  





</body>
</html>
