<!DOCTYPE html>
<html lang="en">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="逻辑回归(Logistic Regression)"/>




  <meta name="keywords" content="ML," />




  <link rel="alternate" href="/atom.xml" title="HuisBlog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.4.x" />



<link rel="canonical" href="http://huisblog.cn/2017/06/03/logisticreg/"/>


<meta name="description" content="分类表达式分类和回归 分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。   分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；   而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。  分类问题的假设表达式逻辑函数(Logistic Function or Si">
<meta name="keywords" content="ML">
<meta property="og:type" content="article">
<meta property="og:title" content="逻辑回归(Logistic Regression)">
<meta property="og:url" content="http://huisblog.cn/2017/06/03/logisticreg/index.html">
<meta property="og:site_name" content="HuisBlog">
<meta property="og:description" content="分类表达式分类和回归 分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。   分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；   而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。  分类问题的假设表达式逻辑函数(Logistic Function or Si">
<meta property="og:image" content="http://huisblog.cn/2017/06/03/logisticreg/sigmoid.png">
<meta property="og:image" content="http://huisblog.cn/2017/06/03/logisticreg/3classes.png">
<meta property="og:updated_time" content="2017-06-06T01:48:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="逻辑回归(Logistic Regression)">
<meta name="twitter:description" content="分类表达式分类和回归 分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。   分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；   而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。  分类问题的假设表达式逻辑函数(Logistic Function or Si">
<meta name="twitter:image" content="http://huisblog.cn/2017/06/03/logisticreg/sigmoid.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.4.x" />







<script>
  var CONFIG = {
    search: true,
    searchPath: "/search.xml",
    fancybox: false,
    toc: true,
  }
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?6e0afd1cf600be3a82f747e8c14cd52d";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




    <title> 逻辑回归(Logistic Regression) · HuisBlog </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">HuisBlog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            Home
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            Archives
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            Tags
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            Categories
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            About
          
        </li>
      </a>
    
      <a href="/friends">
        <li class="mobile-menu-item">
          
          
            Friends
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">HuisBlog</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              Home
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              Archives
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              Tags
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              Categories
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              About
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/friends">
            
            
              Friends
            
          </a>
        </li>
      
      
        <li class="menu-search">
          <form>
            <i class="iconfont icon-search" id="open-search"></i>
            <input type="text" class="search-input" id="search-input" />
            <i class="iconfont icon-close" id="close-search"></i>
          </form>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          逻辑回归(Logistic Regression)
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          Jun 3, 2017
        </span>
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#分类表达式"><span class="toc-text">分类表达式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分类和回归"><span class="toc-text">分类和回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类问题的假设表达式"><span class="toc-text">分类问题的假设表达式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#逻辑函数-Logistic-Function-or-Sigmoid-Function"><span class="toc-text">逻辑函数(Logistic Function or Sigmoid Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#假设函数的物理意义"><span class="toc-text">假设函数的物理意义</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#判定边界-Decision-Boundary"><span class="toc-text">判定边界(Decision Boundary)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逻辑回归模型"><span class="toc-text">逻辑回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数和梯度下降"><span class="toc-text">损失函数和梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#损失函数"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#优化算法"><span class="toc-text">优化算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合-Overfitting-和正则化-Regularization"><span class="toc-text">过拟合(Overfitting)和正则化(Regularization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归的正则化"><span class="toc-text">线性回归的正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#损失函数正则化"><span class="toc-text">损失函数正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降正则化"><span class="toc-text">梯度下降正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#法方程正则化"><span class="toc-text">法方程正则化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑回归的正则化"><span class="toc-text">逻辑回归的正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#正则化后的损失函数"><span class="toc-text">正则化后的损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#正则化后的梯度下降"><span class="toc-text">正则化后的梯度下降</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多值分类"><span class="toc-text">多值分类</span></a></li></ol>
    </div>
  </div>


    <div class="post-content">
      
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h2 id="分类表达式"><a href="#分类表达式" class="headerlink" title="分类表达式"></a>分类表达式</h2><h3 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h3><ul>
<li>分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。  </li>
<li>分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；  </li>
<li>而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。</li>
</ul>
<h3 id="分类问题的假设表达式"><a href="#分类问题的假设表达式" class="headerlink" title="分类问题的假设表达式"></a>分类问题的假设表达式</h3><h4 id="逻辑函数-Logistic-Function-or-Sigmoid-Function"><a href="#逻辑函数-Logistic-Function-or-Sigmoid-Function" class="headerlink" title="逻辑函数(Logistic Function or Sigmoid Function)"></a>逻辑函数(Logistic Function or Sigmoid Function)</h4><p>使用逻辑函数\(g(z)\)将预测值映射到[0, 1]区间中，使假设函数更适用于分类问题。</p>
<p>$$\begin{align}&amp;<br>h_\theta (x) = g ( \theta^T x )<br>\newline<br>\newline&amp; z = \theta^T x<br>\newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}<br>\end{align}$$</p>
<a id="more"></a>
<p>逻辑函数和输入值：</p>
<img src="/2017/06/03/logisticreg/sigmoid.png" alt="sigmoid.png" title="">
<p>可以注意到：</p>
<p>$$\begin{align}z=0, e^{0}=1 \Rightarrow g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align}$$</p>
<h4 id="假设函数的物理意义"><a href="#假设函数的物理意义" class="headerlink" title="假设函数的物理意义"></a>假设函数的物理意义</h4><p>\(h_{\theta}(x)\)表示预测值等于1的可能性：</p>
<p>$$\begin{align}&amp;<br>h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta)<br>\newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1<br>\end{align}$$</p>
<h3 id="判定边界-Decision-Boundary"><a href="#判定边界-Decision-Boundary" class="headerlink" title="判定边界(Decision Boundary)"></a>判定边界(Decision Boundary)</h3><p>判定边界是指分隔开不同类特征点的曲线（面），它由\(h_{\theta}(x)\)假设函数决定。<br>例如下面这个例子（二分类）中，判定边界是直线\(x_1=5\)</p>
<p>$$\begin{align}&amp; \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline &amp; y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline &amp; 5 - x_1 \geq 0 \newline &amp; - x_1 \geq -5 \newline&amp; x_1 \leq 5 \newline \end{align}$$</p>
<p>注意逻辑函数(\(e.g. \theta^Tx\))的输入不一定是线性的，它可以是任何适合训练数据的形状，比如圆(\(e.g. z =\theta_0+\theta_1x_1^2+\theta_2x_2^2\))。</p>
<h2 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h2><h3 id="损失函数和梯度下降"><a href="#损失函数和梯度下降" class="headerlink" title="损失函数和梯度下降"></a>损失函数和梯度下降</h3><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>不能直接使用之前线性回归中使用的损失函数\(J(\theta)=\frac{1}{2m}\sum_{i=0}^{i=m}(h_{\theta}(x_i)-y_i)^2\)，使用逻辑函数会导致其发生扭曲，产生许多局部最优点（极小值），换句话说就不再是凸函数了。<br>所以将损失函数修改为：</p>
<p>$$\begin{align}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align}$$</p>
<p>即：</p>
<p>$$J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$$</p>
<p>向量形式为：</p>
<p>$$\begin{align} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align}$$</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>梯度下降的标准形式：</p>
<p>$$\begin{align}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align}$$</p>
<p>代入以上\(J(\theta)\)得到：</p>
<p>$$\begin{align} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align}$$</p>
<p>向量形式为：</p>
<p>$$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$$</p>
<h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p><code>Conjugate gradient</code>、 <code>BFGS</code>和<code>L-BFG</code>是梯度下降的优化算法，MATLAB有提供这几个库函数，使用这几个函数同样需要定义\(J(\theta)\)和\(\dfrac{\partial}{\partial \theta_j}J(\theta)\)。</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></div><div class="line">  jVal = [...code to compute J(theta)...];</div><div class="line">  gradient = [...code to compute derivative of J(theta)...];</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>MATLAB提供了<code>fminunc()</code>函数来实现优化：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">100</span>);</div><div class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>);</div><div class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</div></pre></td></tr></table></figure>
<h2 id="过拟合-Overfitting-和正则化-Regularization"><a href="#过拟合-Overfitting-和正则化-Regularization" class="headerlink" title="过拟合(Overfitting)和正则化(Regularization)"></a>过拟合(Overfitting)和正则化(Regularization)</h2><p><code>欠拟合(underfitting)</code>或<code>高偏差(high bias)</code>，是指假设函数不足以描述数据趋势，通常由假设函数过于简单或者特征维数太少导致；<br><code>过拟合(overfitting)</code>或<code>高方差(high variance)</code>，是指假设函数对训练集拟合良好但对测试集的通用性差，通常由于假设函数过于复杂或特征维数太高。</p>
<p>解决过拟合问题的常用方法：</p>
<ol>
<li>减少特征维数：手动选择特征、使用模型选择算法；</li>
<li><code>正则化(Regularization)</code>：保留所有特征，但减小参数\(\theta_j\)的量级；正则化在有许多弱相关特征(slightly useful features)的时候很有用。</li>
</ol>
<h3 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h3><h4 id="损失函数正则化"><a href="#损失函数正则化" class="headerlink" title="损失函数正则化"></a>损失函数正则化</h4><p>正则化其实就是通过增加参数在损失函数中的影响来减弱其最后在假设函数中的权重：<br>例如，对于假设函数\(\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4\)，如果想让它变得更加“二次方”，即排除\(\theta_3x^3 \)和\(\theta_4x^4\)的影响，可以通过修改损失函数来实现：</p>
<p>$$min_\theta \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2$$</p>
<p>正则化所有参数（除了\(\theta_0\)）：</p>
<p>$$min_\theta \dfrac{1}{2m}  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n \theta_j^2$$</p>
<p>其中\(\lambda\)是<code>正则化系数(regularization parameter)</code>，\(\lambda\)越大，拟合曲线越平滑，但要注意的是\(\lambda\)过大可能导致欠拟合。</p>
<h4 id="梯度下降正则化"><a href="#梯度下降正则化" class="headerlink" title="梯度下降正则化"></a>梯度下降正则化</h4><p>对于线性回归而言，正则化后的梯度下降：</p>
<p>$$\begin{align} &amp; \text{Repeat} \lbrace \newline &amp;     \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;     \theta_j := \theta_j - \alpha \left[ \left( \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;          j \in \lbrace 1,2…n\rbrace\newline &amp; \rbrace \end{align}$$ </p>
<h4 id="法方程正则化"><a href="#法方程正则化" class="headerlink" title="法方程正则化"></a>法方程正则化</h4><p>正则化后的法方程可以写为：</p>
<p>$$\begin{align}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}  L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align}$$</p>
<p>要注意梯度下降中的下标\(j\)是从1开始的，同样，法方程中\(\lambda(1,1)=0\)，这是因为截取项\(\theta_0\)是不需要正则化的。</p>
<h3 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h3><h4 id="正则化后的损失函数"><a href="#正则化后的损失函数" class="headerlink" title="正则化后的损失函数"></a>正则化后的损失函数</h4><p>逻辑回归的损失函数加上正则化项：</p>
<p>$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)} \log (h_\theta (x^{(i)})) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$</p>
<p>注意\(\sum_{j=1}^n \theta_j^2\)下标从1开始，在梯度下降中也要特别注意\(\theta_0\)的更新。</p>
<h4 id="正则化后的梯度下降"><a href="#正则化后的梯度下降" class="headerlink" title="正则化后的梯度下降"></a>正则化后的梯度下降</h4><p>$$\begin{align} &amp; \text{Repeat} \lbrace \newline &amp;     \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;     \theta_j := \theta_j - \alpha \left[ \left( \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;          j \in \lbrace 1,2…n\rbrace\newline &amp; \rbrace \end{align}$$ </p>
<h2 id="多值分类"><a href="#多值分类" class="headerlink" title="多值分类"></a>多值分类</h2><p>当预测分类值有多个，如\(y={0,1,2,…,n}\)时，可以将其看做\(n+1\)个二值分类问题：对于每一类，将其看做一类而其他所有的看做一类。</p>
<p>$$\begin{align}&amp; y \in \lbrace0, 1 … n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align}$$</p>
<ol>
<li>训练时，对每一类做二值逻辑回归分类器训练；</li>
<li>预测时，计算每类的预测值，最大的为结果。</li>
</ol>
<p>举个例子（3值分类）：</p>
<img src="/2017/06/03/logisticreg/3classes.png" alt="3classes.png" title="">

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>Author: </span>
      <span>Hui Zhang</span>
    </p>
    <p class="copyright-item">
      <span>Origin: </span>
      <a href="http://huisblog.cn">http://huisblog.cn</a>
    </p>
    <p class="copyright-item">
      <span>Link: </span>
      <a href="http://huisblog.cn/2017/06/03/logisticreg/">http://huisblog.cn/2017/06/03/logisticreg/</a>
    </p>

    <p class="copyright-item lincese">
      
      本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
    </p>
  </div>



      
      
    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/ML/">ML</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/06/03/multilinear/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">多变量线性回归</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2017/06/15/neuralnetwork/">
        <span class="next-text nav-default">神经网络(Neural Networks)</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>  
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:helenzhang2015@foxmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a>
  </span>
  
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
     
      2015 - 
    
    2017

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Hui Zhang</span>
  </span>
</div>
      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  

  
  




    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.4.x"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.4.x"></script>

    
  <script type="text/html" id="search-result">
    <article class="post">
      <header class="post-header">
        <h1 class="post-title">
          <a href="$url$" class="post-link">
            $title$
          </a>
        </h1>
      </header>
      <div class="post-content">
        $content$
        <div class="read-more">
          <a href="$url$" class="read-more-link">
            Read more..
          </a>
        </div>
      </div>
    </article>
  </script>
  <script type="text/html" id="no-search-result">
    <div class="no-result">
      <h2>No result found!</h2>
    </div>
  </script>
  <script type="text/javascript" src="/js/src/search.js?v=2.4.x"></script>

  </body>
</html>
