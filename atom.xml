<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>HuisBlog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://huisblog.cn/"/>
  <updated>2018-04-18T12:22:46.545Z</updated>
  <id>http://huisblog.cn/</id>
  
  <author>
    <name>Hui Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基本概念和用法</title>
    <link href="http://huisblog.cn/2018/04/18/PyTorch/PyTorch-1/"/>
    <id>http://huisblog.cn/2018/04/18/PyTorch/PyTorch-1/</id>
    <published>2018-04-18T09:43:40.000Z</published>
    <updated>2018-04-18T12:22:46.545Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch 是基于 Python 的科学计算包，<a href="http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" target="_blank" rel="external">官网</a>给它自己的定义是：</p><blockquote><ol><li>A replacement for NumPy to use the power of GPUs</li><li>A deep learning research platform that provides maximum flexibility and speed</li></ol></blockquote><p>可以看出，PyTorch 提供了一种类似 Numpy 的抽象方法来表征张量（或多维数组），它还能利用 GPU 来提升性能。</p><a id="more"></a><h2 id="基础数据结构"><a href="#基础数据结构" class="headerlink" title="基础数据结构"></a>基础数据结构</h2><p>PyTorch 的关键数据结构是张量 (Tensors)，即多维数组。其功能与 NumPy 的 ndarray 对象类似，不同的是它可以在GPU上使用。</p><ul><li>张量的基本操作可见于<a href="http://pytorch.org/docs/stable/torch.html" target="_blank" rel="external">这里</a></li><li>张量和Numpy的互相转换：  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from tensor to ndarray</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.numpy()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a.add_(<span class="number">1</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(b)</div><div class="line">[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</div><div class="line"></div><div class="line"><span class="comment"># from ndarray to tensor</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.ones(<span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.from_numpy(a)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>numpy.add(a, <span class="number">1</span>, out=a)</div><div class="line">array([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(b)</div><div class="line"> <span class="number">2</span></div><div class="line"> <span class="number">2</span></div><div class="line"> <span class="number">2</span></div><div class="line"> <span class="number">2</span></div><div class="line">[torch.DoubleTensor of size <span class="number">4</span>]</div></pre></td></tr></table></figure><blockquote><p>值得注意的是，Torch Tensor 和 NumPy array 其实是共享内存位置的，因此改变一个另一个也会因此改变。</p></blockquote><ul><li>张量使用GPU加速计算</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Tensors can be moved onto GPU using the .cuda method.</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> torch.cuda.is_available():</div><div class="line"><span class="meta">... </span>    x = x.cuda()</div><div class="line"><span class="meta">... </span>    y = y.cuda()</div><div class="line"><span class="meta">... </span>    print(x+y)</div><div class="line"><span class="meta">... </span></div><div class="line"> <span class="number">0.8296</span>  <span class="number">1.5523</span>  <span class="number">0.8696</span>  <span class="number">1.4905</span></div><div class="line"> <span class="number">0.7956</span>  <span class="number">0.5263</span>  <span class="number">0.6687</span>  <span class="number">1.7677</span></div><div class="line"> <span class="number">1.0483</span>  <span class="number">0.7037</span>  <span class="number">0.8505</span>  <span class="number">1.5983</span></div><div class="line">[torch.cuda.FloatTensor of size <span class="number">3</span>x4 (GPU <span class="number">0</span>)]</div></pre></td></tr></table></figure><h2 id="自动微分库"><a href="#自动微分库" class="headerlink" title="自动微分库"></a>自动微分库</h2><p>自动微分库<code>autograd</code>是 PyTorch 用于所有神经网络计算的关键库，它提供了所有关于张量的微分操作。Autograd 是基于动态框架的，这意味着反向传播根据代码运行状态而改变，每次迭代过程中都可能不同。</p><p><code>autograd.Variable</code> 是这个包中最重要的类。它：</p><ul><li>是张量的简单封装</li><li>记录了所有历史操作</li><li>将梯度保存在 .grad 中</li><li>可以帮助建立计算图</li></ul><p><img src="http://pytorch.org/tutorials/_images/Variable.png"></p><p>每次结束计算后，使用<code>.backward()</code>可以自动计算出所有梯度值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = Variable(x, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</div><div class="line"><span class="meta">... </span>    y = y * <span class="number">2</span></div><div class="line"><span class="meta">... </span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(y)</div><div class="line">Variable containing:</div><div class="line"> <span class="number">908.8007</span></div><div class="line"> <span class="number">549.7198</span></div><div class="line"> <span class="number">507.9812</span></div><div class="line">[torch.FloatTensor of size <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># through .data we can access raw tensor</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(y.data)</div><div class="line"> <span class="number">908.8007</span></div><div class="line"> <span class="number">549.7198</span></div><div class="line"> <span class="number">507.9812</span></div><div class="line">[torch.FloatTensor of size <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># .grad_fn references a Function that has created the Variable </span></div><div class="line"><span class="comment"># (except for Variables created by the user - their grad_fn is None).</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(y.grad_fn)</div><div class="line">&lt;MulBackward0 object at <span class="number">0x7ffa1ea95470</span>&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(x.grad_fn)</div><div class="line"><span class="keyword">None</span></div><div class="line"></div><div class="line"><span class="comment"># we can call .backward() on a Variable to compute the derivatives. </span></div><div class="line"><span class="comment"># If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>gradients = torch.FloatTensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward(gradients)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(x.grad)</div><div class="line">Variable containing:</div><div class="line">  <span class="number">51.2000</span></div><div class="line"> <span class="number">512.0000</span></div><div class="line">   <span class="number">0.0512</span></div><div class="line">[torch.FloatTensor of size <span class="number">3</span>]</div></pre></td></tr></table></figure><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p><code>torch.nn</code>包可以用来构建神经网络。一个<code>nn.Module</code>包含了<code>layers</code>和用于返回<code>output</code>的<code>forward(input)</code> 方法。</p><p>例如这个简单的前馈神经网络：</p><p><img src="http://pytorch.org/tutorials/_images/mnist.png"></p><p>一般典型的神经网络训练步骤如下：</p><ol><li>定义带学习参数 (learnable parameters or weights) 的神经网络； </li><li>输入数据集进行迭代计算；</li><li>使用神经网络处理输入；</li><li>计算损失；</li><li>将梯度返回到网络的参数中；</li><li>更新参数。</li></ol><h3 id="定义神经网络"><a href="#定义神经网络" class="headerlink" title="定义神经网络"></a>定义神经网络</h3><p>定义一个神经网络只需要定义其：</p><ul><li>层结构 (Layers)  </li><li>前馈函数 (Forward function)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">You just have to define the forward function, and the backward function </span></div><div class="line"><span class="string">(where gradients are computed) is automatically defined for you using autograd. </span></div><div class="line"><span class="string">You can use any of the Tensor operations in the forward function.</span></div><div class="line"><span class="string">"""</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line"><span class="comment"># Define the basic layer structure </span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></div><div class="line">        <span class="comment"># kernel</span></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</div><div class="line">        <span class="comment"># an affine operation: y = Wx + b</span></div><div class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</div><div class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="comment"># Define the forward function and automaticly generate backward function</span></div><div class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</div><div class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</div><div class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></div><div class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></div><div class="line">        num_features = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</div><div class="line">            num_features *= s</div><div class="line">        <span class="keyword">return</span> num_features</div><div class="line"></div><div class="line"></div><div class="line">net = Net()</div><div class="line">print(net)</div><div class="line"></div><div class="line"><span class="comment"># The learnable parameters of a model are returned by net.parameters()</span></div><div class="line">params = list(net.parameters())</div><div class="line">print(len(params))</div><div class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></div><div class="line"></div><div class="line"><span class="comment"># ---------------- Output Result: ----------------------</span></div><div class="line">Net(</div><div class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</div><div class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="keyword">True</span>)</div><div class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="keyword">True</span>)</div><div class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="keyword">True</span>)</div><div class="line">)</div><div class="line"><span class="number">10</span></div><div class="line">torch.Size([<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</div></pre></td></tr></table></figure><h3 id="损失函数、反向传播与参数（权重）更新"><a href="#损失函数、反向传播与参数（权重）更新" class="headerlink" title="损失函数、反向传播与参数（权重）更新"></a>损失函数、反向传播与参数（权重）更新</h3><p><code>nn</code>中已经定义了许多<a href="http://pytorch.org/docs/stable/nn.html" target="_blank" rel="external">损失函数</a>，其中一个简单的是<code>nn.MSELoss</code>，用于计算均方误差。</p><p>最简单的权重更新规则是随机梯度下降（Stochastic Gradient Descent, SGD）：<br><code>weight = weight - learning_rate * gradient</code></p><p>同时，<code>torch.optim</code>为更新权重提供了优化算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"></div><div class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)) <span class="comment"># a dummy input, for example</span></div><div class="line">target = Variable(torch.arange(<span class="number">1</span>, <span class="number">11</span>))  <span class="comment"># a dummy target, for example</span></div><div class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># make it the same shape as output</span></div><div class="line"></div><div class="line"><span class="comment"># define the loss function</span></div><div class="line">criterion = nn.MSELoss()</div><div class="line"></div><div class="line"><span class="comment"># create your optimizer</span></div><div class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</div><div class="line"></div><div class="line"><span class="comment"># do the training loop:</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">optimizer.zero_grad() <span class="comment"># Zero-out previous gradients</span></div><div class="line">output = net(input)</div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward() <span class="comment"># Compute new gradients</span></div><div class="line">optimizer.step() <span class="comment"># Apply these gradients and do the update</span></div><div class="line"></div><div class="line"><span class="comment"># Updated parameters are available at net.parameters()</span></div></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://mp.weixin.qq.com/s/FQxSYjyR1U3TIinSShnOfg" target="_blank" rel="external">从头开始了解PyTorch的简单实现</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch 是基于 Python 的科学计算包，&lt;a href=&quot;http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官网&lt;/a&gt;给它自己的定义是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;A replacement for NumPy to use the power of GPUs&lt;/li&gt;
&lt;li&gt;A deep learning research platform that provides maximum flexibility and speed&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以看出，PyTorch 提供了一种类似 Numpy 的抽象方法来表征张量（或多维数组），它还能利用 GPU 来提升性能。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://huisblog.cn/categories/PyTorch/"/>
    
    
      <category term="DL" scheme="http://huisblog.cn/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>记录一些常用的Linux命令</title>
    <link href="http://huisblog.cn/2018/04/12/linux-cmd/"/>
    <id>http://huisblog.cn/2018/04/12/linux-cmd/</id>
    <published>2018-04-12T11:42:17.000Z</published>
    <updated>2018-04-18T05:47:25.852Z</updated>
    
    <content type="html"><![CDATA[<p>本文用于记录一些常用的Linux命令以备忘。</p><a id="more"></a><h2 id="SSH服务相关"><a href="#SSH服务相关" class="headerlink" title="SSH服务相关"></a>SSH服务相关</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">who <span class="comment"># 查看当前登录的用户</span></div><div class="line">who /var/<span class="built_in">log</span>/wtmp <span class="comment"># 查看自从wtmp文件创建以来的每一次登陆情况 </span></div><div class="line"><span class="variable">$HOME</span>/.bash_history <span class="comment"># 查看每个用户的历史命令</span></div><div class="line"><span class="built_in">history</span> <span class="comment">#  当前用户的历史命令</span></div></pre></td></tr></table></figure><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 系统环境变量文件</span></div><div class="line">/etc/profile —— 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置；</div><div class="line">/etc/environment —— 在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量；</div><div class="line">/etc/bashrc —— 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取；</div><div class="line"><span class="comment"># 用户环境变量文件</span></div><div class="line">~/.profile —— 每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量,执行用户的.bashrc文件；</div><div class="line">~/.bashrc —— 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取；</div></pre></td></tr></table></figure><h2 id="防火墙"><a href="#防火墙" class="headerlink" title="防火墙"></a>防火墙</h2><p>Linux高层使用<code>ufw</code>的防火墙为添加或删除简单规则提供了简易的方法，默认情况下，ufw处于禁用状态。几条<code>ufw</code>的常用命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看防火墙状态</span></div><div class="line">sudo ufw status</div><div class="line"><span class="comment"># 开启防火墙，并在系统启动时自动开启。</span></div><div class="line">sudo ufw <span class="built_in">enable</span></div><div class="line"><span class="comment"># 关闭所有外部对本机的访问，但本机访问外部正常。</span></div><div class="line">sudo ufw default deny</div><div class="line"><span class="comment"># 关闭防火墙</span></div><div class="line">sudo ufw <span class="built_in">disable</span></div><div class="line"><span class="comment"># 开启/禁用</span></div><div class="line">sudo ufw allow|deny [service]</div></pre></td></tr></table></figure><p>低层用<code>iptables</code>实现访问控制：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看iptables已有规则</span></div><div class="line">sudo iptables -L -n -v</div><div class="line"><span class="comment"># 开放端口</span></div><div class="line">sudo iptables -A INPUT -p tcp --dport 80 -j ACCEPT</div><div class="line"><span class="comment"># 允许IP访问</span></div><div class="line">sudo iptables -A INPUT -s 10.xx.xx.xx -dport 80 -j ACCEPT</div></pre></td></tr></table></figure><h2 id="网络分析"><a href="#网络分析" class="headerlink" title="网络分析"></a>网络分析</h2><p><code>tcpdump</code>是基于Unix系统的命令行式的数据包嗅探工具，可以抓取流动在网卡上的数据包。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 抓取主机10.37.63.255和主机10.37.63.61或10.37.63.95的通信</span></div><div class="line">tcpdump host 10.37.63.255 and \(10.37.63.61 or 10.37.63.95 \)</div></pre></td></tr></table></figure><p><code>nethogs</code>是一款小巧的”net top”工具，可以显示每个进程所使用的带宽，并对列表排序，将耗用带宽最多的进程排在最上面。万一出现带宽使用突然激增的情况，用户迅速打开nethogs，就可以找到导致带宽使用激增的进程。nethogs可以报告程序的进程编号（PID）、用户和路径。</p><h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><p>[1] <a href="https://help.ubuntu.com/lts/serverguide/firewall.html" target="_blank" rel="external">Ubuntu 16.04 防火墙</a><br>[2] <a href="http://einverne.github.io/post/2017/07/use-nethogs-to-check-network-traffic-per-process.html" target="_blank" rel="external">使用 nethogs 查看每个进程流量 </a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文用于记录一些常用的Linux命令以备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="linux" scheme="http://huisblog.cn/tags/linux/"/>
    
      <category term="cmd" scheme="http://huisblog.cn/tags/cmd/"/>
    
  </entry>
  
  <entry>
    <title>安装PyTorch</title>
    <link href="http://huisblog.cn/2018/04/12/PyTorch/PyTorch-0/"/>
    <id>http://huisblog.cn/2018/04/12/PyTorch/PyTorch-0/</id>
    <published>2018-04-12T11:37:00.000Z</published>
    <updated>2018-04-18T06:56:33.337Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录一下给服务器安装PyTorch的过程。</p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装包管理器Anaconda"><a href="#安装包管理器Anaconda" class="headerlink" title="安装包管理器Anaconda"></a>安装包管理器Anaconda</h3><ol><li>下载<a href="https://docs.anaconda.com/anaconda/install/linux" target="_blank" rel="external">Anaconda for python3.6</a></li><li>上传到服务器<code>scp Anaconda3-5.1.0-Linux-x86_64.sh user@server:~/share</code></li><li>安装<code>bash ~/share/Anaconda3-5.1.0-Linux-x86_64.sh</code></li><li>激活<code>source ~/.bashrc</code></li><li>更新到最新版<code>conda update -n base conda</code></li></ol><blockquote><p>Anaconda 会自动修改环境变量，用户下的python(及pip)将会被切换为python3.6，但不会影响系统使用的python版本：<br>可以使用 sudo su 切换到root再使用 python(python3) –version 查看</p></blockquote><h3 id="更新pip和numpy到最新版本"><a href="#更新pip和numpy到最新版本" class="headerlink" title="更新pip和numpy到最新版本"></a>更新pip和numpy到最新版本</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 更新pip3和numpy</span></div><div class="line">conda update pip</div><div class="line">conda update numpy</div></pre></td></tr></table></figure><h3 id="安装GPU加速工具包"><a href="#安装GPU加速工具包" class="headerlink" title="安装GPU加速工具包"></a>安装GPU加速工具包</h3><p>选择正确版本的<a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html" target="_blank" rel="external">CUDA TOOLKIT</a>和<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">目标主机设置</a>，下载cuda-repo-xx.deb</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install linux-headers-$(uname -r)</div><div class="line"></div><div class="line">sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb</div><div class="line">sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub`</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install cuda</div></pre></td></tr></table></figure><h2 id="开始安装PyTorch"><a href="#开始安装PyTorch" class="headerlink" title="开始安装PyTorch"></a>开始安装PyTorch</h2><p>本来想使用Anaconda安装的，但它的镜像连接不上，改用PIP安装。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install http://download.pytorch.org/whl/cu91/torch-0.3.1-cp36-cp36m-linux_x86_64.whl </div><div class="line">pip install torchvision</div></pre></td></tr></table></figure><blockquote><p>如果由于网络原因安装失败可以先下载下来再 pip install ~/TorchFileLocalPath</p></blockquote><p>安装成功：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; python</div><div class="line">Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) </div><div class="line">[GCC 7.2.0] on linux</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line">&gt;&gt;&gt; import torch</div><div class="line">&gt;&gt;&gt; import torchvision</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure><h3 id="使用Pycharm进行远程开发与调试"><a href="#使用Pycharm进行远程开发与调试" class="headerlink" title="使用Pycharm进行远程开发与调试"></a>使用Pycharm进行远程开发与调试</h3><p>强大的<code>Pycharm Pro</code>IDE可以直接在服务器上开发、调试，就不用再给本地搭建和服务器上一样的环境了。在Tools-&gt;Deployment里面可以设置自动部署（或者在创建项目时选用远程服务器和解释器）。</p><p>需要设置的有<code>SFTP</code>和<code>Remote Interpreter</code>。SFTP用于代码同步，类似git上传，同步后可以在服务器上直接运行上传的代码。</p><ol><li>在Tools-&gt;Deployment中添加SFTP服务：<code>Connect</code>页填写服务器连接选项，<code>Mapping</code>页设置本地路径和远程路径（注意这个路径是相对于前面的Root Path的）的映射；</li><li>选择自动上传(Automatic Uploaded)；</li><li>Preference-&gt;Project Interpreter中添加远程解释器，可以使用已添加的SFTP中的服务器设置（使用Deployment Configure选项）</li></ol><blockquote><p>遇到了问题，Python Console不可用，远程连接控制台不成功：<br>ssh://usr@10.xx.xx.xx:22/home/usr/anaconda3/bin/python -u /home/usr/.pycharm_helpers/pydev/pydevconsole.py 0 0<br>Couldn’t connect to console process.<br>Process finished with exit code -1<br>这是<a href="https://youtrack.jetbrains.com/issue/PY-18029#tab=Comments" target="_blank" rel="external">Pycharm的一个未解决BUG</a>，但评论中提到的解决方法<a href="https://stackoverflow.com/questions/31323363/pycharm-4-5-3-remote-console-cannot-connect-to-remote-process/31323892" target="_blank" rel="external">allowing the server public IP for all traffic</a>对我的情况没用。无法解决，暂搁置。</p></blockquote><p>Python Console 问题后续:</p><ul><li>查看pycharm的log无有用提示（<code>~/Library/Logs/PyCharm2018.1/idea.log</code>）；</li><li>使用<code>nettop</code>监控pycharm进程通信发现有一个通信一直被关闭，以为是进程通信的问题：<img src="/2018/04/12/PyTorch/PyTorch-0/pycharm_tcp_closed.jpeg">  </li><li>给macos开发端口：修改<code>\etc\pf.conf</code>配置文件，但还是没有解决问题。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sudo vim /etc/pf.conf</div><div class="line"></div><div class="line"><span class="comment"># 添加规则允许本机IP访问任何端口</span></div><div class="line">pass <span class="keyword">in</span> proto tcp from 127.0.0.1</div><div class="line">pass <span class="keyword">in</span> proto tcp from localhost</div><div class="line"></div><div class="line"><span class="comment"># 使配置生效</span></div><div class="line">sudo pfctl -evf /etc/pf.conf</div></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://www.jianshu.com/p/988cd2137139" target="_blank" rel="external">pycharm 远程服务器开发调试</a><br>[2] <a href="http://paranoth.me/2018/03/19/Pycharm%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B5%84%E6%BA%90/" target="_blank" rel="external">Pycharm远程调试使用服务器资源</a><br>[3] <a href="https://blog.chionlab.moe/2016/02/01/use-pf-on-osx/" target="_blank" rel="external">OSX上pf的简单配置笔记</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文记录一下给服务器安装PyTorch的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://huisblog.cn/categories/PyTorch/"/>
    
    
      <category term="DL" scheme="http://huisblog.cn/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>给实验室配置共有资源</title>
    <link href="http://huisblog.cn/2018/04/10/happilab-server/"/>
    <id>http://huisblog.cn/2018/04/10/happilab-server/</id>
    <published>2018-04-10T12:31:21.000Z</published>
    <updated>2018-04-10T12:39:07.137Z</updated>
    
    <content type="html"><![CDATA[<p>最近导师新购置了一台工作站，为了让大家可以共享一些文件和计算资源，我在这台工作站上配置了SSH和FTP服务。本文用以记录我的配置过程。</p><a id="more"></a> <h2 id="设置SSH服务器远程登录管理"><a href="#设置SSH服务器远程登录管理" class="headerlink" title="设置SSH服务器远程登录管理"></a>设置SSH服务器远程登录管理</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. 开启服务器上的SSH服务</span></div><div class="line"><span class="comment"># 1-1. 更新软件列表</span></div><div class="line">sudo apt-get update</div><div class="line"><span class="comment"># 1-2. 更新本地软件</span></div><div class="line">sudo apt-get upgrade</div><div class="line"><span class="comment"># 1-3. 安装ssh服务</span></div><div class="line">sudo apt-get install openssh-server</div><div class="line"><span class="comment"># 1-4. 开启ssh服务</span></div><div class="line">sudo /etc/init.d/ssh start</div><div class="line"></div><div class="line"><span class="comment"># 2. 添加公钥认证</span></div><div class="line"><span class="comment"># 在本地机器上生成公私钥对</span></div><div class="line">ssh-keygen</div><div class="line"><span class="comment"># 复制公钥到远程服务器上</span></div><div class="line">ssh-copy-id [-i your_id_rsa.pub] &lt;username&gt;@&lt;server IP&gt;</div><div class="line"><span class="comment"># 之后可以直接使用ssh [-i your_id_rsa] &lt;username&gt;@&lt;server IP&gt;登录</span></div></pre></td></tr></table></figure><h2 id="配置FTP文件共享服务"><a href="#配置FTP文件共享服务" class="headerlink" title="配置FTP文件共享服务"></a>配置FTP文件共享服务</h2><p>使用<code>vsftpd</code>来开启FTP服务器：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. 下载并安装`vsftpd`</span></div><div class="line">sudo apt-get install vsftpd</div><div class="line"></div><div class="line"><span class="comment"># 2. 创建可登陆用户账户及文件目录和权限</span></div><div class="line"><span class="comment"># 2-1. 在ubuntu中创建新用户happiftp和密码******</span></div><div class="line"><span class="comment"># 2-2. 将新用户的登录目录/home/happiftp作为FTP文件根目录</span></div><div class="line"><span class="comment"># 2-3. 在/home/happiftp下创建public文件夹给匿名用户使用</span></div><div class="line"><span class="comment"># 2-4. 设置登录根目录及实际可操作目录的权限[1]</span></div><div class="line">sudo chmod 500 /home/happiftp</div><div class="line">sudo chmod 500 /home/happiftp/public</div><div class="line">sudo chmod 777 /home/happiftp/HappiLabFiles</div><div class="line">sudo chmod 777 /home/happiftp/public/upload</div><div class="line"></div><div class="line"><span class="comment"># 3. 修改vsftpd配置文件</span></div><div class="line">[...]</div><div class="line">utf8_filesystem=YES</div><div class="line"></div><div class="line"><span class="comment"># 使用本地账户登录的设置</span></div><div class="line">local_root=/home/happiftp/</div><div class="line">local_enable=YES</div><div class="line">write_enable=YES</div><div class="line">local_umask=077</div><div class="line"><span class="comment"># 限制登录用户不能进入到上级目录</span></div><div class="line">chroot_local_user=YES</div><div class="line"><span class="comment">#chroot_list_enable=YES</span></div><div class="line"><span class="comment">#chroot_list_file=/etc/vsftpd.chroot_list</span></div><div class="line"></div><div class="line"><span class="comment"># 匿名用户设置</span></div><div class="line">anon_root=/home/happiftp/public/</div><div class="line">anonymous_enable=YES</div><div class="line">anon_upload_enable=YES</div><div class="line">anon_mkdir_write_enable=YES</div><div class="line">anon_umask=022</div><div class="line"></div><div class="line"><span class="comment"># 重启vsftpd服务</span></div><div class="line">sudo service vsftpd restart</div></pre></td></tr></table></figure><blockquote><p>[1] 出于安全考虑，vsftpd不支持将具有完全权限的文件夹作为FTP登录的根目录。如果出现这种情况，FTP客户端将无法登录：<br>500 OOPS: vsftpd: refusing to run with writable root inside chroot()  </p></blockquote><!--## 配置多个虚拟机账户--><h2 id="配置学校网络链接"><a href="#配置学校网络链接" class="headerlink" title="配置学校网络链接"></a>配置学校网络链接</h2><p>这样设置完成后还只能从实验室内网登录这些服务，但我希望大家能连接校网就能使用，这样就需要设置一下实验室路由器了。</p><ol><li>给工作站主机保留ip地址，使其在实验室内网中的ip不会变化：局域网IP设置-&gt;地址保留-&gt;添加地址保留；</li><li>需要开放端口映射或者设置DMZ主机，让校网用户可以访问实验室工作站。由于DMZ主机会开放主机的所有端口，现使用端口映射：端口映射/端口触发-&gt;添加FTP端口映射（内外部端口都设置为20-21）和SSH端口映射（内外部端口都设置为22）；</li><li>这时校网用户可以通过路由器的222.xx.xx.xx（zjuvpn提供的ip）访问主机服务，但这个ip是学校动态分配的，一直会变。。同时，无法通过10.xx.xx.xx静态IP访问主机，推测是因为使用这个IP时候路由器的信号传入和传出走的是不同的路线使得不通，所以要设置静态路由来指定信号传输地址：静态路由-&gt;添加3条地址分别为10.0.0.0，210.xx.xx.0，222.xx.0.0（网关10.xx.xx.1）的静态路由表。</li></ol><blockquote><p>大概是计网知识已经还给老师了，自己各种搞不定路由设置，这里超感谢我的朋友@<a href="https://blog.hlyue.com/" target="_blank" rel="external">Richard</a>帮忙。</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="http://blog.sina.com.cn/s/blog_816caa410101kvpx.html" target="_blank" rel="external">端口映射和DMZ主机</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近导师新购置了一台工作站，为了让大家可以共享一些文件和计算资源，我在这台工作站上配置了SSH和FTP服务。本文用以记录我的配置过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="server" scheme="http://huisblog.cn/tags/server/"/>
    
  </entry>
  
  <entry>
    <title>2018-2019 相关会议投稿时间</title>
    <link href="http://huisblog.cn/2018/03/30/conference/"/>
    <id>http://huisblog.cn/2018/03/30/conference/</id>
    <published>2018-03-30T06:32:17.000Z</published>
    <updated>2018-03-30T07:11:53.541Z</updated>
    
    <content type="html"><![CDATA[<p>“计算机科学的publication最大特点在于：极度重视会议，而期刊则通常只用来做re-publication”。本文整理了2018年下半年到2019年上半年我们实验室感兴趣领域的一些可投会议及时间节点。</p><a id="more"></a><blockquote><p>大部分期刊文章都是会议论文的扩展版，首发就在期刊上的相对较少。也正因为如此，计算机期刊的影响因子都低到惊人的程度，顶级刊物往往也只有1到2左右（被引的通常都是会议版论文，而不是很久以后才出版的期刊版）。因此，要讨论计算机科学的publication，首先必须强调的一点是totally forget about IF (IF指影响因子)。</p></blockquote><h2 id="图形学与多媒体"><a href="#图形学与多媒体" class="headerlink" title="图形学与多媒体"></a>图形学与多媒体</h2><table><thead><tr><th>名称</th><th>CCF</th><th>EI收录</th><th>时间</th><th>投稿时间</th></tr></thead><tbody><tr><td><a href="http://dblp.uni-trier.de/db/conf/mm/" target="_blank" rel="external">ACM MM</a></td><td>A</td><td></td><td>2018-08</td><td><del>2018-03</del></td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/siggraph/index.html" target="_blank" rel="external">SIGGRAPH</a></td><td>A</td><td></td><td>2018-08</td><td><del>2018-01</del></td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/mir/" target="_blank" rel="external">ICMR</a></td><td>B</td><td></td><td>2018-06</td><td><del>2018-01</del></td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/icmcs/" target="_blank" rel="external">ICME</a></td><td>B</td><td></td><td>2018-06</td><td><del>2017-11</del></td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/icassp/" target="_blank" rel="external">ICASSP</a></td><td>B</td><td></td><td>2018-04</td><td><del>2017-10</del></td></tr><tr><td><a href="http://www.ismir.net/" target="_blank" rel="external">ISMIR</a></td><td>无</td><td>无</td><td>2018-07</td><td><del>2018-03</del></td></tr><tr><td><a href="https://icmc2018.org/" target="_blank" rel="external">ICMC</a></td><td>无</td><td></td><td>2018-08</td><td><del>2018-03</del></td></tr></tbody></table><h2 id="人机交互与普适计算"><a href="#人机交互与普适计算" class="headerlink" title="人机交互与普适计算"></a>人机交互与普适计算</h2><table><thead><tr><th>名称</th><th>CCF</th><th>EI收录</th><th>时间</th><th>投稿时间</th></tr></thead><tbody><tr><td><a href="https://chi2018.acm.org/authors/papers/" target="_blank" rel="external">CHI</a></td><td>A</td><td></td><td>2019-05</td><td><code>2018-09</code></td></tr><tr><td><a href="https://cscw.acm.org/2018/submit/papers.html" target="_blank" rel="external">CSCW</a></td><td>A</td><td></td><td>2018-11</td><td>2018-04</td></tr><tr><td><a href="https://cscw.acm.org/2018/submit/papers.html" target="_blank" rel="external">IUI</a></td><td>B</td><td></td><td>2019-03</td><td><code>2018-10</code></td></tr></tbody></table><h2 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h2><table><thead><tr><th>名称</th><th>CCF</th><th>EI收录</th><th>时间</th><th>投稿时间</th></tr></thead><tbody><tr><td><a href="http://dblp.uni-trier.de/db/conf/aaai/" target="_blank" rel="external">AAAI</a></td><td>A</td><td></td><td>2019-02</td><td>TBD</td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/icml/" target="_blank" rel="external">ICML</a></td><td>A</td><td></td><td>2019-06</td><td>TBD</td></tr><tr><td><a href="https://nips.cc/Conferences/2018" target="_blank" rel="external">NIPS</a></td><td>A</td><td></td><td>TBD</td><td>TBD</td></tr><tr><td><a href="http://dblp.uni-trier.de/db/conf/ijcai/" target="_blank" rel="external">IJCAI</a></td><td>A</td><td></td><td>TBD</td><td>TBD</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“计算机科学的publication最大特点在于：极度重视会议，而期刊则通常只用来做re-publication”。本文整理了2018年下半年到2019年上半年我们实验室感兴趣领域的一些可投会议及时间节点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="publication" scheme="http://huisblog.cn/tags/publication/"/>
    
  </entry>
  
  <entry>
    <title>《TDD - Python Web》学习笔记</title>
    <link href="http://huisblog.cn/2018/03/23/web-tdd/"/>
    <id>http://huisblog.cn/2018/03/23/web-tdd/</id>
    <published>2018-03-23T04:15:09.000Z</published>
    <updated>2018-03-27T10:25:57.467Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.obeythetestinggoat.com/" target="_blank" rel="external">Test-Driven-Development for Python Web 开源学习链接</a>中有本书的最新版内容以及相关讨论，非常值得阅读学习。<br><a id="more"></a></p><h2 id="TDD中的重要概念"><a href="#TDD中的重要概念" class="headerlink" title="TDD中的重要概念"></a>TDD中的重要概念</h2><h3 id="自动化测试金字塔"><a href="#自动化测试金字塔" class="headerlink" title="自动化测试金字塔"></a>自动化测试金字塔</h3><p>单元测试-&gt;组件测试(-&gt;集成测试-&gt;系统测试-&gt;人工探索式测试)。单元测试和验收测试首先是文档，然后才是测试。它们当然可以验证系统是否达到了具体指标，但它们更为重要的目的是如实描述系统的设计、结构和行为。</p><ul><li>单元测试：测试大多数异常路径。可以说是程序员写给程序员的<code>正式设计文档</code>，描述低层结构以及代码行为；</li><li>功能测试（/组件测试/验收测试）：测试成功路径、极端情况、边界状态和可选路径。业务方和QA一起完成的<code>正式需求文档</code>，描述系统功能；</li><li>集成测试：测试大型系统中各组件间的正常通信（各组件是否协调）；</li><li>系统测试：最终的集成测试，测试系统是否已正确组装完毕。</li></ul><h3 id="测试的不同类型"><a href="#测试的不同类型" class="headerlink" title="测试的不同类型"></a>测试的不同类型</h3><ul><li><p>隔离测试（或者说纯粹的单元测试）与整合测试(integrated tests)<br>单元测试的主要作用是验证应用的逻辑是否正确。隔离测试是纯粹的单元测试，它只测试一部分代码，且只有这部分代码（比如一个函数）能够让测试失败。而如果这个函数依赖于其他系统且破坏这个系统会导致测试失败，就说明这是整合测试。</p></li><li><p>集成测试(Integration tests)<br>集成测试用于检查被你控制的代码是否和你无法控制的外部系统完好集成。集成测试往往也是整合测试。</p></li><li><p>系统测试<br>如果说集成测试检查的是与外部系统的集成情况，那么系统测试就是检查应用内部多个系统间的集成情况。例如：检查数据库、静态文件和服务器配置在一起是否能正常运行。</p></li></ul><h2 id="网站开发中的有趣知识点"><a href="#网站开发中的有趣知识点" class="headerlink" title="网站开发中的有趣知识点"></a>网站开发中的有趣知识点</h2><ul><li><p>用户故事：从用户的角度描述应用该如何运行，用来组织功能测试。</p></li><li><p>预期失败：意料之中的失败，驱动开发。</p></li><li><p>“不测试常量”规则：单元测试的规则之一是“不测试常量”——单元测试要测试的其实是逻辑、流程控制和配置。</p></li><li><p>重构：重构是指在功能不变的前提下改进代码，其首要原则是不能没有测试（保证重构前后的表现一致）。记住，重构时，修改代码或者测试，但不能同时修改。</p></li><li><p>模板：模板是<code>Django</code>中一个很强大的功能，它能把Python变量代入HTML文本；还能使用模板标签来使用模板句法：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#123;&#123; var &#125;&#125; - 引入Python变量</div><div class="line">&#123;% block replaceable_part %&#125;&#123;% endblock %&#125; - 模板标签</div></pre></td></tr></table></figure><ul><li><p><code>单元测试/编写代码</code>循环有时也叫<code>遇红/变绿/重构</code>：1. 先编写一个会失败的单元测试（遇红）；2. 编写尽可能简单的代码让测试通过（变绿），就算作弊也行；3. 重构，改进代码使其更合理，可以用三角法判断什么时候应该重构代码。</p></li><li><p><code>三角法</code>：添加一个测试，专门为现有的某些代码编写用例，以此推断出普适的实现方式（之前的实现方法可能作弊了）。</p></li><li><p>事不过三，三则重构</p></li><li><p>回归：新添加的代码破坏了原本可正常使用的功能</p></li><li><p>记在便签上的待办事项清单：在便签上记录编写代码过程中遇到的问题，等手头的工作完成后再回过头来解决</p></li><li><p><code>YAGNI</code>：You aint gonna need it.</p></li><li><p>Django对撇号(apostrophe)会自动转码(HTML-escaped)</p></li><li><p>Django可以把每个模型对象与特定页面(URL)相关联</p></li><li><p>使用硬编码的URL是不科学的</p></li><li><p><code>mocking</code>：可以在单元测试中使用模拟技术测试外部依赖(如API)，它可以避免重复和测试其他人的代码，但在使用过程中要记得用<code>patch</code>修饰器避免副作用。但<code>mock</code>太多可能会导致代码异味（它依赖于实现方式），要避免使用太多。另外还要注意模拟的对象在<code>if语句</code>中的表现可能有违常规，<code>mock</code>对象是一个正值，而且可以掩盖错误，因为它有所有的属性和方法！</p></li><li><p>Python类的动态性质：它们在运行时创建，并可以在创建之后进一步修改。 </p></li><li><p>Python<code>修饰器</code>：装饰器本质上的作用就是为已经存在的对象添加额外的功能。装饰器的顺序：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@a</span></div><div class="line"><span class="meta">@b</span></div><div class="line"><span class="meta">@c</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span> <span class="params">()</span>:</span></div><div class="line"><span class="keyword">pass</span></div><div class="line"><span class="comment"># 等价于 f = a(b(c(f)))</span></div></pre></td></tr></table></figure><ul><li><code>Test Fixtures</code>-测试固件，是指使用测试数据预先填充数据库的过程，例如储存User对象及其相关的Session(会话)对象。值得注意的是要在Django中避免使用JSON格式的固件，因为一旦修改了模型，这种固件的维护将会变成一个噩梦…（这时推荐使用Django ORM或者factory_boy之类的工具）</li></ul><ul><li><code>持续集成(Continuous Integration/CI)</code>服务器搭建过程：  </li></ul><ol><li>获得<code>Jenkins</code>服务器：获取一个具有控制权的服务器 -&gt; 安装最新的Jenkins -&gt; 配置Jenkins的安全设置；</li><li>安装插件和设置项目：安装插件（部署项目前需要安装的依赖）并设置虚拟显示器、失败截图和转储HTML（还要安装PhantomJS运行QUnit JavaScript测试） -&gt; 设置项目 -&gt; 构建项目；</li><li>Bonus：把CI和过渡服务器连接起来：使用CI服务器部署代码到过渡服务器并在过渡服务器中运行功能测试。</li></ol><blockquote><p>记录一个在练习部署的时候踩到的一个大坑！  </p></blockquote><p>在第21章的练习中，要求将“发送邮件登录”功能部署到服务器上，这个功能在本地的开发服务器上测试成功，设置如下：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># setting.py</span></div><div class="line">EMAIL_HOST = <span class="string">'smtp.qq.com'</span></div><div class="line">EMAIL_HOST_USER = <span class="string">'my_qq_num@qq.com'</span></div><div class="line">EMAIL_HOST_PASSWORD = os.environ.get(<span class="string">'EMAIL_PASSWORD'</span>)</div><div class="line">EMAIL_PORT = <span class="number">25</span></div><div class="line">EMAIL_USE_TLS = <span class="keyword">True</span></div></pre></td></tr></table></figure><p>结果部署后，过渡服务器上的功能测试失败，邮件无法发送。服务器日志中未显示任何error，只是提示WORKER TIMEOUT。比较搞笑的是，最开始以为是环境变量的问题，以为gunicorn没有读到我的EMAIL_PASSWORD（因为gunicorn在systemd中设置的EnviromentFile只为ExecStart下的进程服务，在shell中echo不出来），这个导致浪费了很多时间。最后<a href="https://stackoverflow.com/questions/1950442/connection-timeout-issue-sending-email-in-django" target="_blank" rel="external">Connection timeout issue sending email in Django</a>中的回答给了我提示：可能是发件服务器端口的问题：</p><p>于是尝试<code>server$ telnet smtp.qq.com 25</code>得不到连接结果，发现是我的服务器无法连接到QQ邮箱服务器25端口造成的<code>send_mail</code>超时失败。。改用465端口及SSL加密设置，测试成功通过：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># setting.py</span></div><div class="line">[...]</div><div class="line">EMAIL_PORT = <span class="number">465</span></div><div class="line">EMAIL_USE_SSL = <span class="keyword">True</span></div></pre></td></tr></table></figure><h2 id="拓展学习"><a href="#拓展学习" class="headerlink" title="拓展学习"></a>拓展学习</h2><ul><li>Gunicorn</li><li>MVC（模型-试图-控制器）</li><li>REST（表现层状态转化）</li></ul><h2 id="学习收获"><a href="#学习收获" class="headerlink" title="学习收获"></a>学习收获</h2><p>读完一本书，其中技巧性的东西如果在平常不使用就会被淡忘，最重要的是理解和吸收书中的思想。</p><p>当初我是带着几个问题来学习这本书的：</p><ol><li>什么是测试驱动开发？</li><li>为什么要用测试驱动开发？</li><li>怎样在开发的过程运用TDD的思想？</li><li>TDD在网站开发中的具体方法？</li></ol><p>现在大概可以一一解答了：</p><ol><li>在编写可以真正实现功能的代码前，首先编写测试。只有在测试和预期一样失败后才能继续进行下一步。</li><li>测试不仅仅可以保障代码的功能，给我们修改代码的勇气，它更是一种文档，可以帮助我们实现更好的设计。比如功能测试是应用的说明书，它会提供一个人类可读、容易理解的故事，帮助我们开发具有所需功能的应用（还能保证我们不会无意中破坏这些功能）；而单元测试则描述了代码希望实现的效果（应用中的每行代码都应该至少有一个单元测试），告诉我们应该怎样编写代码—— 功能测试站在高层驱动开发，单元测试从底层驱动我们做些什么。而且TDD还使得我们永远不会忘记接下来应该做什么——重新运行测试就知道要做的事了。总而言之，重视测试的终极原因是测试让开发变得更有乐趣。</li><li>使用遇红-&gt;变绿-&gt;重构的流程进行开发：<img src="/2018/03/23/web-tdd/tdd.png"></li><li>使用“由外而内的TDD”完成页面。其实上面双循环的TDD流程就体现了由外而内的思想：从外部设计系统，再分层编写代码。具体到网站开发中，就是：从表现层(GUI，模板与URL)逐步向内层移动，通过视图层或控制器层，最终到达模型层。这种方法的理念是由实际需要使用的功能驱动代码的编写，而不是在低层猜测需求。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.obeythetestinggoat.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Test-Driven-Development for Python Web 开源学习链接&lt;/a&gt;中有本书的最新版内容以及相关讨论，非常值得阅读学习。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="http://huisblog.cn/tags/python/"/>
    
      <category term="web" scheme="http://huisblog.cn/tags/web/"/>
    
  </entry>
  
  <entry>
    <title>使用Magenta生成音乐</title>
    <link href="http://huisblog.cn/2018/03/20/magenta-intro/"/>
    <id>http://huisblog.cn/2018/03/20/magenta-intro/</id>
    <published>2018-03-20T13:44:39.000Z</published>
    <updated>2018-03-20T14:16:34.724Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始入门计算机自动作曲领域（<code>算法作曲(algorithmic composition)</code>或称<code>自动作曲(automated composition)</code>）。计算机自动生成音乐的目标有节奏和旋律、和声/和弦、伴奏(acompaniment)、复调(counterpoint)、改编(arrangement)等。它的实现一般都是基于概率理论的（或者说其数学本质就是一个概率问题）：假设旋律（一段包含音高的时间序列）服从某种概率分布，求解这个分布的近似解（或者加上和声，求解它们的联合概率分布模型）。</p><p><a href="https://github.com/tensorflow/magenta/blob/master/README.md#using-magenta" target="_blank" rel="external">Magenta</a> 是Google Brain团队使用深度学习研究自动作曲的开源项目，本文将简单描述使用Magenta生成MIDI音乐的过程。</p><a id="more"></a><h2 id="安装和配置支持Python3的Magenta环境"><a href="#安装和配置支持Python3的Magenta环境" class="headerlink" title="安装和配置支持Python3的Magenta环境"></a>安装和配置支持Python3的Magenta环境</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 安装[Miniconda for python3](https://conda.io/miniconda.html)</span></div><div class="line">$ sh Miniconda3-latest-MacOSX-x86_64.sh</div><div class="line"></div><div class="line"><span class="comment"># 创建Magenta的conda环境</span></div><div class="line">$ conda create -n magenta python=3.6 jupyter</div><div class="line"></div><div class="line"><span class="comment"># 激活环境</span></div><div class="line">$ <span class="built_in">source</span> activate magenta</div><div class="line"></div><div class="line"><span class="comment"># 安装pip包</span></div><div class="line">pip3 install magenta</div></pre></td></tr></table></figure><h2 id="使用Melody-RNN模型生成旋律"><a href="#使用Melody-RNN模型生成旋律" class="headerlink" title="使用Melody RNN模型生成旋律"></a>使用Melody RNN模型生成旋律</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 下载配置文件[basic_rnn/lookback_rnn/attention_rnn](https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn)</span></div><div class="line"></div><div class="line">$ <span class="built_in">source</span> activate magenta</div><div class="line">$ BUNDLE_PATH=&lt;absolute path of .mag file&gt;</div><div class="line">$ CONFIG=&lt;one of <span class="string">'basic_rnn'</span>, <span class="string">'lookback_rnn'</span>, or <span class="string">'attention_rnn'</span>, matching the bundle&gt;</div><div class="line"></div><div class="line">$ melody_rnn_generate \</div><div class="line">--config=<span class="variable">$&#123;CONFIG&#125;</span> \</div><div class="line">--bundle_file=<span class="variable">$&#123;BUNDLE_PATH&#125;</span> \</div><div class="line">--output_dir=/tmp/melody_rnn/generated \</div><div class="line">--num_outputs=10 \</div><div class="line">--num_steps=128 \</div><div class="line">--primer_melody=<span class="string">"[60]"</span></div><div class="line"></div><div class="line"><span class="comment"># num_outputs表示生成旋律的数量</span></div><div class="line"><span class="comment"># num_steps表示以16分音符为单位的旋律长度</span></div><div class="line"><span class="comment"># primer_melody是输入的初始音高（如：小星星[60, -2, 60, -2, 67, -2, 67, -2]）</span></div><div class="line"><span class="comment"># primer_melody可以被primer_midi替换：--primer_midi=&lt;absolute path to magenta/models/melody_rnn/primer.mid&gt;</span></div></pre></td></tr></table></figure><h2 id="播放和评估MIDI效果"><a href="#播放和评估MIDI效果" class="headerlink" title="播放和评估MIDI效果"></a>播放和评估MIDI效果</h2><p>安装<a href="https://github.com/FluidSynth/fluidsynth" target="_blank" rel="external">FluidSynth</a>MIDI合成器:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ brew install fluidsynth</div></pre></td></tr></table></figure><p>并下载<a href="https://musescore.org/zh-hans/node/36171#sf2-soundfonts" target="_blank" rel="external">SoundFont</a>，为MIDI合成器提供乐器录音和音效。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ fluidsynth /path/to/soundfont.sf2 /path/to/MIDI.mid</div></pre></td></tr></table></figure><blockquote><p>或者其实可以直接使用<a href="https://musescore.org/zh-hans/download" target="_blank" rel="external">musescore</a>来播放，好处是有GUI可以直接可视化乐谱！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近开始入门计算机自动作曲领域（&lt;code&gt;算法作曲(algorithmic composition)&lt;/code&gt;或称&lt;code&gt;自动作曲(automated composition)&lt;/code&gt;）。计算机自动生成音乐的目标有节奏和旋律、和声/和弦、伴奏(acompaniment)、复调(counterpoint)、改编(arrangement)等。它的实现一般都是基于概率理论的（或者说其数学本质就是一个概率问题）：假设旋律（一段包含音高的时间序列）服从某种概率分布，求解这个分布的近似解（或者加上和声，求解它们的联合概率分布模型）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/magenta/blob/master/README.md#using-magenta&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Magenta&lt;/a&gt; 是Google Brain团队使用深度学习研究自动作曲的开源项目，本文将简单描述使用Magenta生成MIDI音乐的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Music Generation" scheme="http://huisblog.cn/categories/Music-Generation/"/>
    
    
      <category term="music" scheme="http://huisblog.cn/tags/music/"/>
    
      <category term="midi" scheme="http://huisblog.cn/tags/midi/"/>
    
      <category term="magenta" scheme="http://huisblog.cn/tags/magenta/"/>
    
  </entry>
  
  <entry>
    <title>基本乐理知识</title>
    <link href="http://huisblog.cn/2018/03/12/basic-music/"/>
    <id>http://huisblog.cn/2018/03/12/basic-music/</id>
    <published>2018-03-12T13:08:42.000Z</published>
    <updated>2018-03-12T14:28:43.955Z</updated>
    
    <content type="html"><![CDATA[<h2 id="音的四种性质"><a href="#音的四种性质" class="headerlink" title="音的四种性质"></a>音的四种性质</h2><p>音乐是由音构成的，而音作为一种物理现象，是由于物体的振动而产生的。物体振动产生音波，音波通过空气作用于我们的听觉器官并传送至大脑，我们就产生音的感觉。在自然界中存在着各种各样许许多多种声音，这些声音我们有的能听到，有的则听不到。我们人耳所能听到的声音，大致在每秒钟振动11—20000次的范围之内，而在音乐中所使用的音，一般只限于每秒振动27—4100次这个范围之内，而且大都是易于分辨的有限的一些音。根据音的物理属性，音有四种性质，即：</p><ul><li><code>音高(Pitch)</code></li><li><code>时值(Note value)</code></li><li><code>强弱(Dynamics)</code></li><li><code>音色(Timbre)</code></li></ul><p>音的四种性质，在音乐表现中，都有着充分的体现，如由于音色的不同，我们才能区分各种不同的乐器和人声。由于音有高有低，有长有短，有强有弱，我们才能写出丰富多彩、多种多样优美的旋律和动听的和声。但在音的四种性质中，音的高低和长短却有着更加突出的重要作用。比如，一支旋律无论大声唱或小声哼，用小提琴拉或用小号吹，它的基本形象并不会有什么大的改变，若将音高或长短稍加变动，音乐形象立刻就会不同程度地被破坏。</p><a id="more"></a><h2 id="音乐的三要素"><a href="#音乐的三要素" class="headerlink" title="音乐的三要素"></a>音乐的三要素</h2><ul><li><code>节奏(rhythm)</code></li><li><code>旋律(melody)</code></li><li><code>和声(harmony)</code></li></ul><h3 id="节拍与节奏"><a href="#节拍与节奏" class="headerlink" title="节拍与节奏"></a>节拍与节奏</h3><p>在说节奏之前，先来聊一聊<code>节拍(beat)</code>。有强有弱的相同的时间片断，按照一定的次序循环重复，就叫做节拍；节拍中的每一时间片断，叫做<code>单位拍</code>，也就是我们通常说的“一拍”，它是描述音乐的时间单位。</p><img src="/2018/03/12/basic-music/音乐的时间单位.png"><p>在节拍的每一循环中，只有一个强音时，带强音的单位拍，就叫做“强拍”。不带强音的单位拍，就叫做“弱拍”。</p><img src="/2018/03/12/basic-music/强弱拍.png"><p>在节拍的每一循环中，不只一个强音时，第一个带强音的单位拍叫做“强拍”；其他带强音的单位拍，叫“次强拍”；不带强音的单位拍，叫做“弱拍”。</p><img src="/2018/03/12/basic-music/强弱拍2.png"><p>单位拍可以用各种基本音符来代表，而表示拍子的记号，叫做“拍号”。拍号用分数的形式来标记：分子表示节拍的每一循环中有几拍，分母表示以什么音符为一拍。如：每一循环有两拍，以四分音符为一拍，这就叫做“四二拍子”。</p><img src="/2018/03/12/basic-music/拍号.png"><p>在音乐进行中，音与音之间长短关系与强弱关系所组成的序列叫做<code>节奏</code>，它是一个较高级的概念。换句话说，音乐中的“节奏”实际上涵盖了除音高与音色之外的一切因素。比如：音的时值长短，音的强弱，音乐的拍子、重音，小节，节奏音型等等这诸多因素，都包含在节奏这一概念之中。在介绍节奏的同时，还有一个值得一提的概念就是节奏型。在音乐作品中，具有典型意义和特定性格的节奏称做<code>节奏型</code>。比如下图中，四分音符+两个八分音符+两个四分音符构成了一个节奏型，这个节奏型连续出现了两次。</p><img src="/2018/03/12/basic-music/节奏型.png"><p>舞蹈就是一门建立在音乐的节奏之上的一门艺术。许多与舞蹈同名的音乐风格在节奏上都有鲜明的特征，他们都有各自的速度、拍型和节奏型。拉丁舞蹈和音乐的风格有伦巴，桑巴，恰恰等等，大多都是偶数拍子（如二拍子，四拍子）的舞蹈。恰恰舞在练习时有“踏<del>踏</del>恰恰恰！”的口诀帮助初学者踩对节奏，这便是节奏型的体现。伦巴是表现男女爱情的舞蹈，速度因此相对舒缓以承托柔美的舞蹈。而一提起桑巴，我们则会想起穿着如孔雀般鲜艳的巴西妹子热情洋溢的快节奏舞蹈，桑巴是二拍子的。</p><h3 id="音符与旋律"><a href="#音符与旋律" class="headerlink" title="音符与旋律"></a>音符与旋律</h3><p>音乐中的单词就叫做<code>音符(Note)</code>，而音符在时间轴上有序地连接在一起时就出现了<code>旋律(Melody)</code>。下图是一段简化了的五线谱。旋律中的音符随着时间的推移，先是维持在一个固定的音高，接着突然上升了一个较大的音程，接着缓缓下行直到又出现一个向上的跳渡，最后维持在一个固定的音高结束。每一个音符都是由两个元素构成的：<code>音高</code>和<code>时值</code>。音高决定了音符自身频率的高低也决定了音符之间互相的音高关系（即<code>音程</code>），而时值则是指音符的持续时间，时长越长的音符时值就越大。</p><img src="/2018/03/12/basic-music/音符与旋律.png"><p>旋律是能给听众带来意义感的。大脑喜欢探究事物的意义和规则，而我们的听觉又非常灵敏，它能够辨识出音符走向。这使得我们具备了一个神奇的能力，能够像玩连线游戏一样，将分散的音符串联起来，听出有意义的旋律。所以，我们也习惯把旋律叫做“旋律线”。一段旋律必定包含其节奏，永远不会存在只有旋律线而没有任何节奏的音乐作品。</p><blockquote><p>关于节拍节奏和旋律之前的区别，下图解说得很不错：</p></blockquote><img src="/2018/03/12/basic-music/区别.png"><h3 id="和弦与和声"><a href="#和弦与和声" class="headerlink" title="和弦与和声"></a>和弦与和声</h3><p>音阶中的音横向发展得到旋律，而纵向发展就得到了<code>和声(Harmony)</code>。现代音乐概念里面，由多个不同的音高同时发声就叫和声。如果说旋律是音乐中的语句，那么和声就是多个人同时在讲话，或者是某人在表演一嘴多舌的口技了。和声可以扮演音乐中的形容词副词，为旋律线增添五彩缤纷的色彩。</p><img src="/2018/03/12/basic-music/和弦与和声.png"><p>三个或三个音以上的和声就叫做<code>和弦(Chord)</code>。一般来讲，和声的音符数量越多，其色彩就会越丰富。和弦的音程关系远比双音复杂，多个音之间互相都会构成音程，音程组合的可能性随音高数量呈指数增长。在过去的民谣和早期的古典乐中，和弦往往会用到最基础的由三个音组成的和弦，叫做三和弦。而在现代流行音乐中，和声的色彩随着音乐的进化已经更为丰富，和声中往往会在三和弦的架构上再加入一到两个色彩音。爵士乐则更进一步，这个风格的音乐中由五个六个组成音的和弦是家常便饭，即使是钢琴也需要两只手才能把和弦的音按全。</p><p>和弦中的音不仅可以同时弹奏形成和声，也可以先后连续奏出，这个过程叫做和弦的分解，或分解和弦。在弹奏分解和弦时，我们依旧能隐约或强烈地感受到和声。由于大脑的短期记忆可以储存前几秒之前的音符并将这些音符拼接在一起，我们能够感受到分解音之间的音程关系，还原出和声的色彩。同样，我们对旋律的感受也少不了大脑对相邻几个音之间的音程关系的处理。如果没有了短期记忆的帮助，我们恐怕就无法听懂音乐了。</p><h2 id="单调音乐、复调音乐与主调音乐"><a href="#单调音乐、复调音乐与主调音乐" class="headerlink" title="单调音乐、复调音乐与主调音乐"></a>单调音乐、复调音乐与主调音乐</h2><p>音乐三要素之间的结合方式叫做织体<code>(Texture)</code>。就像织布有粗细薄厚之分，我们也可以感受到音乐织体的粗细与薄厚。虽然织体在乐曲中会不断变化，不过还是可以归纳出以3种基本的模式：</p><ul><li><code>单调音乐(Monophony)</code></li><li><code>复调音乐(Polyphony)</code></li><li><code>主调音乐(Homophony)</code></li></ul><p>像下图中由一根旋律线单独进行地音乐织体叫做单调音乐。</p><img src="/2018/03/12/basic-music/单调音乐.png"><p>由多个独立的旋律线并行构成的织体叫做复调音乐。一个乐器即可以演奏复调音乐，但并不是多个乐器或多个声部演奏的音乐就一定是复调音乐。比如，如果所有乐器声部都在演奏和模仿同一个旋律，这种织体被叫做支音音乐。</p><img src="/2018/03/12/basic-music/复调音乐.png"><p>以上这些织体并不是流行乐中最常见的，我们最常见的织体叫做主调音乐。主调音乐织体是由一条旋律线配以和声进行来构成的。旋律中的音符既是和弦中的某个音，同时又可以构成一条独立的旋律。旋律音常常是和声中的高音，但这并不是绝对的，旋律也可以在低音或中音发展。</p><img src="/2018/03/12/basic-music/主调音乐.png"><p>与复调音乐不同的是，主调音乐和声中的非旋律音是用来衬托旋律并为其增色的，它们并不独立构成新的旋律线。而在复调音乐中，多条独立的旋律弦之间却可以互相构成和声关系，同时起到和声与旋律功能，这样的写作手法叫做对位法。由于对乐理知识要求较高，对位法一般是音乐专业学习者才会接触的功课，却也是严肃音乐中必不可少的技能和训练。</p><h2 id="计算机音乐创作与标准MIDI文件"><a href="#计算机音乐创作与标准MIDI文件" class="headerlink" title="计算机音乐创作与标准MIDI文件"></a>计算机音乐创作与标准MIDI文件</h2><p>在计算机音乐创作中，计算机声音合成与计算机作曲是两个最重要的领域，二者既有相似之处，也有本质上的区别。前者提供了声音合成的取舍方法，后者是在作曲过程中由计算机做出相应的决定；前者可看作传统电子音乐技术手段和思维的延续和发展，后者则可看做对人类纸笔谱曲方式的拓展；前者中由计算机输出的主要是声音，如<code>WAV</code>文件或者<code>AIFF</code>文件，而后者中由计算机输出的结果则主要是乐谱或者可由制谱软件读取的文件，如<code>MIDI</code>文件或者<code>MusicXML</code>文件。最近，基于MIDI的计算机音乐自动生成研究已成为主流。</p><p>乐器数字接口（Musical Instrument Digital Interface，简称MIDI）是一个工业标准的电子通信协议，为电子乐器等演奏设备（如合成器）定义各种音符或弹奏码，容许电子乐器、电脑、手机或其它的舞台演出配备彼此连接，调整和同步，得以即时交换演奏数据。MIDI不发送声音，只发送像是音调和音乐强度的数据，音量，颤音和相位等参数的控制信号，还有设置节奏的时钟信号。在不同的电脑上，输出的声音也因音源器不同而有差异。可以说MIDI文件就是电子乐器在看的电子乐谱，所以通常一个文件只需几十KB，就能够让电子乐器演奏出一首很完整的音乐。</p><h3 id="标准MIDI文件结构"><a href="#标准MIDI文件结构" class="headerlink" title="标准MIDI文件结构"></a>标准MIDI文件结构</h3><p><a href="http://www.ccarh.org/courses/253/handout/smf/" target="_blank" rel="external">这份斯坦福大学课程讲义</a>详细描述了标准MIDI文件(Standard MIDI File 简称<code>SMF</code>)的文件结构。SMF是由MIDI块（chunks）构成的。第一个MIDI块是Header块，接下来是一个或多个Track块。Header块包含整个MIDI文件的全局数据。每一个Track块都定义了一个逻辑音轨(track)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SMF = &lt;header_chunk&gt; + &lt;track_chunk&gt; [+ &lt;track_chunk&gt; ...]</div></pre></td></tr></table></figure><p>一个块由三部分构成，与微软的RIFF文件类似（不同之处在于SMF是使用大端法存储(Big-endian)，而RIFF是使用小端法(little-endian)），这三部分定义如下：</p><ol><li>前四个字节是块ID，Header块是”MThd”，Track块是”MTrk”；</li><li>下面四字节是无符号值，用来定义块中数据部分的长度；</li><li>最后则是块数据。</li></ol><h4 id="Header块"><a href="#Header块" class="headerlink" title="Header块"></a>Header块</h4><p>Header块包括块ID、长度、MIDI文件的格式信息、MIDI音轨（指逻辑音轨，即Track块个数）数量和MIDI最小时间单位长度信息。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">header_chunk = &quot;MThd&quot; + &lt;header_length&gt; + &lt;format&gt; + &lt;n&gt; + &lt;division&gt;</div></pre></td></tr></table></figure><ul><li><code>MThd</code>：4字节，标志Header块的字符，16进制表示为：0x4D546864。这4个字符出现在MIDI文件的开头，标志这是一个MIDI文件；</li><li><code>header_length</code>：4字节，指示Header块中数据部分的长度（永远为6字节长，因为数据部分的三个字段均为2字节）；</li><li><code>format</code>：2字节，0表示单一音轨文件格式，1表示多音轨文件格式，2表示多歌曲文件格式（一组单一音轨的文件）；</li><li><code>n</code>：2字节，记录Header块后的Track块的数量；</li><li><code>division</code>：2字节，间隔时间所对应的单位时间数(tick数)。如果这个值为正，那么它标志着每一拍所对应的单位时间数。比如说，+96表示每拍对应96 ticks。如果这个值为负，间隔时间则对应SMPTE单位。</li></ul><h4 id="Track块"><a href="#Track块" class="headerlink" title="Track块"></a>Track块</h4><p>Track块包括块ID、长度和事件信息(event data)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">track_chunk = &quot;MTrk&quot; + &lt;length&gt; + &lt;track_event&gt; [+ &lt;track_event&gt; ...]</div></pre></td></tr></table></figure><ul><li><code>MTrk</code>：4字节，标志Track块开始；</li><li><code>length</code>：4字节，标志数据部分的长度；</li><li><code>track_event</code>：序列化的音轨事件。</li></ul><h4 id="音轨事件-Track-Event"><a href="#音轨事件-Track-Event" class="headerlink" title="音轨事件(Track Event)"></a>音轨事件(Track Event)</h4><p>一个音轨事件由与上一事件的间隔时间和三种事件之一构成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">track_event = &lt;v_time&gt; + &lt;midi_event&gt; | &lt;meta_event&gt; | &lt;sysex_event&gt;</div></pre></td></tr></table></figure><ul><li><code>v_time</code>：一个变长值，用来表示与前一事件的间隔时间；</li><li><code>midi_event</code>：MIDI的通道事件，比如说音符开始(note-on)与音符结束(note-off)。其播放方法在MIDI装置中都相同；</li><li><code>meta_event</code>：元事件；</li><li><code>sysex_event</code>：系统独有事件。</li></ul><h3 id="Python下的MIDI读写处理工具"><a href="#Python下的MIDI读写处理工具" class="headerlink" title="Python下的MIDI读写处理工具"></a>Python下的MIDI读写处理工具</h3><table><thead><tr><th>工具包</th><th>支持的MIDI功能</th></tr></thead><tbody><tr><td><a href="https://github.com/vishnubob/python-midi" target="_blank" rel="external">python-midi</a></td><td>读、写</td></tr><tr><td><a href="https://github.com/olemb/mido/" target="_blank" rel="external">mido</a></td><td>读、写</td></tr><tr><td><a href="https://sourceforge.net/projects/pymidi/" target="_blank" rel="external">pyMIDI</a></td><td>读、写</td></tr><tr><td><a href="http://web.mit.edu/music21/" target="_blank" rel="external">music21</a></td><td>读、写</td></tr><tr><td><a href="http://www.pygame.org/docs/ref/music.html" target="_blank" rel="external">pygame</a></td><td>读、写、播放</td></tr><tr><td><a href="https://www.mellowood.ca/mma/" target="_blank" rel="external">MMA</a></td><td>读、写、播放</td></tr></tbody></table><blockquote><p>更多可见<a href="https://wiki.python.org/moin/PythonInMusic" target="_blank" rel="external">Python音乐大礼包</a></p></blockquote><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 李重光. (2004). 基本乐理通用教材 (Vol. 43). 北京: 高等教育出版社.<br>[2] 乐理101-什么是音乐?. Retrieved March 12, 2018, from <a href="https://zhuanlan.zhihu.com/p/25057274" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25057274</a>.<br>[3] 倪朝晖. (2015). 算法作曲理论与实践. 西南师范大学出版社.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;音的四种性质&quot;&gt;&lt;a href=&quot;#音的四种性质&quot; class=&quot;headerlink&quot; title=&quot;音的四种性质&quot;&gt;&lt;/a&gt;音的四种性质&lt;/h2&gt;&lt;p&gt;音乐是由音构成的，而音作为一种物理现象，是由于物体的振动而产生的。物体振动产生音波，音波通过空气作用于我们的听觉器官并传送至大脑，我们就产生音的感觉。在自然界中存在着各种各样许许多多种声音，这些声音我们有的能听到，有的则听不到。我们人耳所能听到的声音，大致在每秒钟振动11—20000次的范围之内，而在音乐中所使用的音，一般只限于每秒振动27—4100次这个范围之内，而且大都是易于分辨的有限的一些音。根据音的物理属性，音有四种性质，即：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;音高(Pitch)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;时值(Note value)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;强弱(Dynamics)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;音色(Timbre)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;音的四种性质，在音乐表现中，都有着充分的体现，如由于音色的不同，我们才能区分各种不同的乐器和人声。由于音有高有低，有长有短，有强有弱，我们才能写出丰富多彩、多种多样优美的旋律和动听的和声。但在音的四种性质中，音的高低和长短却有着更加突出的重要作用。比如，一支旋律无论大声唱或小声哼，用小提琴拉或用小号吹，它的基本形象并不会有什么大的改变，若将音高或长短稍加变动，音乐形象立刻就会不同程度地被破坏。&lt;/p&gt;
    
    </summary>
    
    
      <category term="music" scheme="http://huisblog.cn/tags/music/"/>
    
      <category term="midi" scheme="http://huisblog.cn/tags/midi/"/>
    
  </entry>
  
  <entry>
    <title>Weka Simple Introduction</title>
    <link href="http://huisblog.cn/2017/12/23/Weka/"/>
    <id>http://huisblog.cn/2017/12/23/Weka/</id>
    <published>2017-12-23T05:42:49.000Z</published>
    <updated>2017-12-23T05:45:20.619Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p><a href="https://www.cs.waikato.ac.nz/~ml/weka/" target="_blank" rel="external">Weka</a>是一个轻量级的开源数据挖掘工具（used in GUI or called from code），在简单使用一些机器学习算法或者进行一些实验性的探索时很好用。<br><a id="more"></a></p><h2 id="Weka-GUI操作简介"><a href="#Weka-GUI操作简介" class="headerlink" title="Weka GUI操作简介"></a>Weka GUI操作简介</h2><p>打开<code>Weka</code>图形界面后会看到5个应用：</p><ul><li>Explorer</li><li>Experiment</li><li>KnowledgeFlow</li><li>Workbench</li><li>Simple CLI</li></ul><h3 id="Explorer"><a href="#Explorer" class="headerlink" title="Explorer"></a>Explorer</h3><p>在<code>Explorer</code>中即可完成整套机器学习任务：</p><ol><li><code>Preprocess</code>：选择和调整数据</li><li><code>Classify</code>：训练和测试学习模型</li><li><code>Cluster</code>：学习数据聚类</li><li><code>Associate</code>：学习联合数据</li><li><code>Select attributes</code>：选择最相关的数据属性</li><li><code>Visualize</code>：可视化2维数据图像</li></ol><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>方便用户创建、执行、修改和分析实验：可以在多个数据集上执行多个算法并分析效果。</p><ol><li><code>Setup</code>:设置实验环境</li><li><code>Run</code>: 执行实验</li><li><code>Analysis</code>：分析实验评估结果</li></ol><h3 id="KnowledgeFlow"><a href="#KnowledgeFlow" class="headerlink" title="KnowledgeFlow"></a>KnowledgeFlow</h3><p><code>KnowledgeFlow</code>是<code>Explorer</code>的替代方案，为<code>Weka</code>中的核心算法提供了图形化前端。</p><h3 id="Simple-CLI"><a href="#Simple-CLI" class="headerlink" title="Simple CLI"></a>Simple CLI</h3><p><code>Simple CLI</code>是一个简单的<em>weka shell</em>，可以在其最后的文本框中输入执行<em>weka command</em>。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.waikato.ac.nz/~ml/weka/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Weka&lt;/a&gt;是一个轻量级的开源数据挖掘工具（used in GUI or called from code），在简单使用一些机器学习算法或者进行一些实验性的探索时很好用。&lt;br&gt;
    
    </summary>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Node更新到8.0+导致的一些问题</title>
    <link href="http://huisblog.cn/2017/10/31/updatenode/"/>
    <id>http://huisblog.cn/2017/10/31/updatenode/</id>
    <published>2017-10-31T05:19:00.000Z</published>
    <updated>2017-10-31T06:06:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近更新了<code>Node.js</code>，结果在使用<code>hexo new post</code>的时候开始报错：<br><a id="more"></a></p><h3 id="报错一-dyld"><a href="#报错一-dyld" class="headerlink" title="报错一 dyld"></a>报错一 dyld</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicui18n.58.dylib</div><div class="line">  Referenced from: /usr/local/bin/node</div><div class="line">  Reason: image not found</div><div class="line">Abort trap: 6</div></pre></td></tr></table></figure><p>重新安装<code>node</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew reinstall node --without-icu4c</div></pre></td></tr></table></figure><p>成功解决！</p><h3 id="报错二-DTraceProviderBindings-node"><a href="#报错二-DTraceProviderBindings-node" class="headerlink" title="报错二 DTraceProviderBindings.node"></a>报错二 DTraceProviderBindings.node</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">Error: The module &apos;/usr/local/lib/node_modules/hexo-cli/node_modules/dtrace-provider/build/Release/DTraceProviderBindings.node&apos;</div><div class="line">was compiled against a different Node.js version using</div><div class="line">NODE_MODULE_VERSION 51. This version of Node.js requires</div><div class="line">NODE_MODULE_VERSION 57. Please try re-compiling or re-installing</div><div class="line">the module (for instance, using `npm rebuild` or `npm install`).</div><div class="line">    at Object.Module._extensions..node (module.js:641:18)</div><div class="line">    at Module.load (module.js:531:32)</div><div class="line">    at tryModuleLoad (module.js:494:12)</div><div class="line">    at Function.Module._load (module.js:486:3)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/dtrace-provider/dtrace-provider.js:17:23)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">    at Object.Module._extensions..js (module.js:623:10)</div><div class="line">    at Module.load (module.js:531:32)</div><div class="line">    at tryModuleLoad (module.js:494:12)</div><div class="line">    at Function.Module._load (module.js:486:3)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/bunyan/lib/bunyan.js:79:18)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">&#123; Error: Cannot find module &apos;./build/default/DTraceProviderBindings&apos;</div><div class="line">    at Function.Module._resolveFilename (module.js:513:15)</div><div class="line">    at Function.Module._load (module.js:463:25)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/dtrace-provider/dtrace-provider.js:17:23)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">    at Object.Module._extensions..js (module.js:623:10)</div><div class="line">    at Module.load (module.js:531:32)</div><div class="line">    at tryModuleLoad (module.js:494:12)</div><div class="line">    at Function.Module._load (module.js:486:3)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/bunyan/lib/bunyan.js:79:18)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">    at Object.Module._extensions..js (module.js:623:10)</div><div class="line">    at Module.load (module.js:531:32) code: &apos;MODULE_NOT_FOUND&apos; &#125;</div><div class="line">&#123; Error: Cannot find module &apos;./build/Debug/DTraceProviderBindings&apos;</div><div class="line">    at Function.Module._resolveFilename (module.js:513:15)</div><div class="line">    at Function.Module._load (module.js:463:25)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/dtrace-provider/dtrace-provider.js:17:23)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">    at Object.Module._extensions..js (module.js:623:10)</div><div class="line">    at Module.load (module.js:531:32)</div><div class="line">    at tryModuleLoad (module.js:494:12)</div><div class="line">    at Function.Module._load (module.js:486:3)</div><div class="line">    at Module.require (module.js:556:17)</div><div class="line">    at require (internal/module.js:11:18)</div><div class="line">    at Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/hexo-cli/node_modules/bunyan/lib/bunyan.js:79:18)</div><div class="line">    at Module._compile (module.js:612:30)</div><div class="line">    at Object.Module._extensions..js (module.js:623:10)</div><div class="line">    at Module.load (module.js:531:32) code: &apos;MODULE_NOT_FOUND&apos; &#125;</div></pre></td></tr></table></figure><p><code>DTraceProviderBindings</code>针对的<code>node</code>版本错误，刚开始以为是<code>node_module</code>没有更新，因此重新安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ rm -rf node_modules</div><div class="line">$ npm install</div><div class="line">$ hexo clean</div></pre></td></tr></table></figure><p>没有解决，继续报错。。尝试重新安装<code>hexo命令行工具</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install -g hexo-cli</div></pre></td></tr></table></figure><p>成功解决！</p><h3 id="报错三-DeprecationWarning"><a href="#报错三-DeprecationWarning" class="headerlink" title="报错三 DeprecationWarning"></a>报错三 DeprecationWarning</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(node:14238) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated.</div></pre></td></tr></table></figure><p><code>DeprecationWarning</code>函数已经弃用，使用<code>--debug</code>参数调试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo version --debug  </div><div class="line">DEBUG Plugin loaded: hexo-deployer-git                                     </div><div class="line">[DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated.</div></pre></td></tr></table></figure><p>发现是<code>hexo-deployer-git</code>的问题，更新这个模块：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm update hexo-deployer-git</div></pre></td></tr></table></figure><p>没有成功，删除包再重新安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ npm uninstall hexo-deployer-git</div><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure><p>成功解决！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近更新了&lt;code&gt;Node.js&lt;/code&gt;，结果在使用&lt;code&gt;hexo new post&lt;/code&gt;的时候开始报错：&lt;br&gt;
    
    </summary>
    
      <category term="博客" scheme="http://huisblog.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
  </entry>
  
  <entry>
    <title>音乐情感数据集</title>
    <link href="http://huisblog.cn/2017/10/30/emodatasets/"/>
    <id>http://huisblog.cn/2017/10/30/emodatasets/</id>
    <published>2017-10-30T12:02:29.000Z</published>
    <updated>2017-12-07T06:23:50.037Z</updated>
    
    <content type="html"><![CDATA[<p>本文收集了一些带情感标签的音乐数据库：<br><a id="more"></a></p><table><thead><tr><th>DATASET</th><th>META DATA</th><th>CONTENTS</th><th>WITH AUDIO</th></tr></thead><tbody><tr><td><a href="https://amg1608.blogspot.ca" target="_blank" rel="external">Amg1608</a></td><td>valence &amp; arousal</td><td>1608 excerpts (30s)</td><td>no</td></tr><tr><td><a href="http://cvml.unige.ch/databases/DEAM" target="_blank" rel="external">DEAM</a></td><td>valence &amp; arousal</td><td>1802 excerpts</td><td>yes</td></tr><tr><td><a href="http://www.eecs.qmul.ac.uk/mmv/datasets/deap/readme.html" target="_blank" rel="external">DEAPDataset</a></td><td>valence &amp; arousal &amp; dominance &amp; physiological data</td><td>120 music video excerpts</td><td>no</td></tr><tr><td><a href="http://cvml.unige.ch/databases/emoMusic" target="_blank" rel="external">emoMusic</a></td><td>arousal &amp; valence</td><td>744 excerpts (45s)</td><td>yes</td></tr><tr><td><a href="http://www.projects.science.uu.nl/memotion/emotifydata/" target="_blank" rel="external">Emotify</a></td><td>induced emotion</td><td>400 excerpts</td><td>yes</td></tr><tr><td><a href="https://hilab.di.ionio.gr/en/music-information-research/" target="_blank" rel="external">GMD</a></td><td>genre &amp; valence &amp; arousal</td><td>1400 songs</td><td>downloadable</td></tr><tr><td><a href="http://csea.phhp.ufl.edu/media/iadsmessage.html" target="_blank" rel="external">IADS</a></td><td>valence &amp; arousal &amp; dominance</td><td>111 sound snippets</td><td>yes</td></tr><tr><td><a href="https://github.com/johnglover/modal" target="_blank" rel="external">MOODetector:Bi-Modal</a></td><td>lyrics &amp; valence &amp; arousal</td><td>133 excerpts</td><td>yes</td></tr><tr><td><a href="https://github.com/johnglover/modal" target="_blank" rel="external">MOODetector:Multi-Modal</a></td><td>lyrics &amp; MIDI &amp; mood</td><td>903 excerpts (30s)</td><td>yes</td></tr><tr><td><a href="http://music.ece.drexel.edu/research/emotion/moodswingsturk" target="_blank" rel="external">moodswings</a></td><td>arousal &amp; valence</td><td>240 excerpts (30s)</td><td>no</td></tr><tr><td><a href="https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/projects2/past-projects/coe/materials/emotion/soundtracks/Index" target="_blank" rel="external">SoundTracks</a></td><td>valence &amp; energy &amp; tension &amp; mood</td><td>360 +110 excerpts</td><td>yes</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文收集了一些带情感标签的音乐数据库：&lt;br&gt;
    
    </summary>
    
    
      <category term="dataset" scheme="http://huisblog.cn/tags/dataset/"/>
    
      <category term="MIR" scheme="http://huisblog.cn/tags/MIR/"/>
    
  </entry>
  
  <entry>
    <title>FFmpeg实用操作</title>
    <link href="http://huisblog.cn/2017/10/24/ffmpeg/"/>
    <id>http://huisblog.cn/2017/10/24/ffmpeg/</id>
    <published>2017-10-24T03:53:55.000Z</published>
    <updated>2017-10-24T03:54:56.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p>最近的实验中有许多音频处理的工作，使用<code>FFmpeg</code>较为频繁，本文用于记录实验中使用过的操作（<a href="https://ffmpeg.org/ffmpeg-utils.html#time-duration-syntax" target="_blank" rel="external">官方文档</a>）以及其中遇到的一些问题（感谢<a href="http://davidaq.com/tutorial/2014/11/20/ffmpeg-commands.html" target="_blank" rel="external">DavidAQ</a>的答疑解惑）。<br><a id="more"></a></p><h3 id="转格式"><a href="#转格式" class="headerlink" title="转格式"></a>转格式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> ffmpeg -i Input.mp3 Output.wav</span></div></pre></td></tr></table></figure><h3 id="截取"><a href="#截取" class="headerlink" title="截取"></a>截取</h3><p>官网提供的命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> ffmpeg -i Input.mp3 -ss Starttime -t Duration Output.mp3</span></div></pre></td></tr></table></figure><p>但是经过FFmpeg处理的音频文件，在苹果系统(OSX, IOS)以及苹果的播放器(ITunes, QuickTime)上往往会显示错误的长度时间。这是<code>FFmpeg</code>的Bug，需要添加参数<code>-write_xing</code>规避：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> ffmpeg -i Input.mp3 -write_xing 0 -ss Starttime -t Duration Output.mp3</span></div></pre></td></tr></table></figure><p>或者指定音频编码器直接复制原来的编码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> ffmpeg -ss Starttime -i Input.mp3 -t Duration -acodec copy Output.mp3</span></div></pre></td></tr></table></figure><p>把<code>-ss</code>提到<code>-i</code>前面作为输入文件的处理参数，这样会先跳转到<code>Starttime</code>再开始解码，而原来的会从开始解码然后丢弃掉<code>Starttime</code>之前的结果，同时<code>-acodec copy</code>表示音频的编码不会发生改变，这样会大大提高速度。</p><blockquote><p>注意处理文件如果不是图片，不要让输入文件与输出文件相同，覆盖后会出现神奇的Bug。</p></blockquote><h3 id="淡入（淡出）"><a href="#淡入（淡出）" class="headerlink" title="淡入（淡出）"></a>淡入（淡出）</h3><p><code>FFmpeg</code>的音频过滤器可以实现这一效果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> ffmpeg -i Input.mp3 -write_xing 0 -af afade=t=<span class="keyword">in</span>:ss=0:d=15 Output.mp3</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;最近的实验中有许多音频处理的工作，使用&lt;code&gt;FFmpeg&lt;/code&gt;较为频繁，本文用于记录实验中使用过的操作（&lt;a href=&quot;https://ffmpeg.org/ffmpeg-utils.html#time-duration-syntax&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;）以及其中遇到的一些问题（感谢&lt;a href=&quot;http://davidaq.com/tutorial/2014/11/20/ffmpeg-commands.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;DavidAQ&lt;/a&gt;的答疑解惑）。&lt;br&gt;
    
    </summary>
    
    
      <category term="ffmpeg" scheme="http://huisblog.cn/tags/ffmpeg/"/>
    
  </entry>
  
  <entry>
    <title>Python GUI</title>
    <link href="http://huisblog.cn/2017/09/27/pygui/"/>
    <id>http://huisblog.cn/2017/09/27/pygui/</id>
    <published>2017-09-27T06:39:06.000Z</published>
    <updated>2017-09-27T06:41:15.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p>最近的实验需要做一个桌面小程序来收集实验信息，决定选用跨平台的<code>Python</code>来写。<br>在<code>Python图形界面编程</code>中常用的工具包有：</p><a id="more"></a><ul><li><a href="https://www.riverbankcomputing.com/software/pyqt" target="_blank" rel="external">PyQt</a> (Python和Qt库的成功融合)</li><li><a href="https://www.wxpython.org/" target="_blank" rel="external">wxPython</a> (wxWidgets的Python封装)</li><li><a href="https://wiki.python.org/moin/TkInter/" target="_blank" rel="external">Tkinter</a> (Python的标准GUI工具包)</li></ul><p>这次使用的是<code>PyQt</code>（相关文档和资料比较多）。</p><h2 id="PyQt"><a href="#PyQt" class="headerlink" title="PyQt"></a>PyQt</h2><p>使用Qt开发程序可以从<code>Qt Widgets</code>或/和<code>Qt Quick</code>开始（<a href="http://blog.csdn.net/liang19890820/article/details/54141552" target="_blank" rel="external">Qt Widgets、QML、Qt Quick的区别</a>）。我们对实验工具的界面要求不高，可以直接使用<code>Qt Widgets</code>开发。它有几个重要的概念：</p><p>C1. UI 界面实现</p><blockquote><p>QApplication</p><blockquote><p>QWidget</p></blockquote></blockquote><p>C2. 组件通信</p><blockquote><p>Signal &amp; Slot （信号槽机制）</p></blockquote><h2 id="使用-PyInstaller-打包可执行程序"><a href="#使用-PyInstaller-打包可执行程序" class="headerlink" title="使用 PyInstaller 打包可执行程序"></a>使用 PyInstaller 打包可执行程序</h2><h3 id="注意在打包后系统路径会发生变化"><a href="#注意在打包后系统路径会发生变化" class="headerlink" title="注意在打包后系统路径会发生变化"></a>注意在打包后系统路径会发生变化</h3><p>平时几种常用的获取当前运行脚本路径的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">print(__file__)</div><div class="line">print(os.path.realpath(__file__))</div><div class="line">print(<span class="string">'using sys.executable:'</span>, repr(os.path.dirname(os.path.realpath(sys.executable))))</div><div class="line">print(<span class="string">'using sys.argv[0]:'</span>, repr(os.path.dirname(os.path.realpath(sys.argv[<span class="number">0</span>]))))</div><div class="line">print(os.path.split(sys.argv[<span class="number">0</span>]))</div><div class="line">print(sys.path[<span class="number">0</span>])</div></pre></td></tr></table></figure><p>在工程中的运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">/Users/huizhang/Desktop/testpath/path.py</div><div class="line">/Users/huizhang/Desktop/testpath/path.py</div><div class="line">using sys.executable: <span class="string">'/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/bin'</span></div><div class="line">using sys.argv[<span class="number">0</span>]: <span class="string">'/Users/huizhang/Desktop/testpath'</span></div><div class="line">(<span class="string">'/Users/huizhang/Desktop/testpath'</span>, <span class="string">'path.py'</span>)</div><div class="line">/Users/huizhang/Desktop/testpath</div></pre></td></tr></table></figure><p>打包后的运行结果：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">path.py</div><div class="line">/Users/huizhang/path.py</div><div class="line">using sys.executable: <span class="string">'/Users/huizhang/Desktop/testpath/dist/path.app/Contents/MacOS'</span></div><div class="line">using sys.argv[0]: <span class="string">'/Users/huizhang/Desktop/testpath/dist/path.app/Contents/MacOS'</span></div><div class="line">(<span class="string">'/Users/huizhang/Desktop/testpath/dist/path.app/Contents/MacOS'</span>, <span class="string">'path'</span>)</div><div class="line">/Users/huizhang/Desktop/testpath/dist/path.app/Contents/MacOS/base_library.zip</div></pre></td></tr></table></figure><h3 id="打包数据文件"><a href="#打包数据文件" class="headerlink" title="打包数据文件"></a>打包数据文件</h3><p><code>pyinstaller</code>命令不能直接将工程中的数据文件一起打包，要实现这一步必须修改<code>.spec</code>文件。<a href="http://legendtkl.com/2015/11/06/pyinstaller/" target="_blank" rel="external">legendtkl的博客</a>中有<code>PyInstaller</code>的简介中文教程，也可以直接查看<a href="https://pythonhosted.org/PyInstaller/spec-files.html" target="_blank" rel="external">官方文档</a>。</p><p>添加数据文件只需要在<code>a.datas</code>里面添加二元组即可，二元组第一个参数<code>&#39;/mygame/data&#39;</code>是要添加的数据文件的本地索引，第二个参数<code>&#39;data&#39;</code>是在打包后的工程中的位置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">added_files = [</div><div class="line">         ( &apos;/mygame/data&apos;, &apos;data&apos; ),</div><div class="line">         ( &apos;/mygame/sfx/*.mp3&apos;, &apos;sfx&apos; ),</div><div class="line">         ( &apos;src/README.txt&apos;, &apos;.&apos; )</div><div class="line">         ]</div><div class="line">         </div><div class="line">    a = Analysis(...</div><div class="line">         datas = added_files,</div><div class="line">         ...</div><div class="line">         )</div></pre></td></tr></table></figure><p><code>PyInstaller</code>会把打包的位置存在<code>sys._MEIPASS</code>，可以测试一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">img = os.path.join(<span class="string">"res"</span>,<span class="string">"img"</span>,<span class="string">"test.jpg"</span>)</div><div class="line"></div><div class="line">base_path0 = os.path.abspath(<span class="string">"."</span>)</div><div class="line">base_path1 = os.path.dirname(os.path.realpath(sys.argv[<span class="number">0</span>]))</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    base_path2 = sys._MEIPASS</div><div class="line"><span class="keyword">except</span> Exception:</div><div class="line">    <span class="comment"># PyInstaller打包前sys._MEIPASS不存在</span></div><div class="line">    base_path2 = os.path.abspath(<span class="string">"."</span>)</div><div class="line"></div><div class="line">print(<span class="string">"img0: "</span>,os.path.join(base_path0,img))</div><div class="line">print(<span class="string">"img1: "</span>,os.path.join(base_path1,img))</div><div class="line">print(<span class="string">"img2: "</span>,os.path.join(base_path2,img))</div></pre></td></tr></table></figure><p>工程中的运行结果都一样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 路径都正确</span></div><div class="line">img0:  /Users/huizhang/Desktop/testinstaller/res/img/test.jpg </div><div class="line">img1:  /Users/huizhang/Desktop/testinstaller/res/img/test.jpg</div><div class="line">img2:  /Users/huizhang/Desktop/testinstaller/res/img/test.jpg</div></pre></td></tr></table></figure><p>打包后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># img0错误，img1和img2正确</span></div><div class="line">img0:  /Users/huizhang/res/img/test.jpg </div><div class="line">img1:  /Users/huizhang/Desktop/testinstaller/dist/test.app/Contents/MacOS/res/img/test.jpg</div><div class="line">img2:  /Users/huizhang/Desktop/testinstaller/dist/test.app/Contents/MacOS/res/img/test.jpg</div></pre></td></tr></table></figure><h3 id="开始打包"><a href="#开始打包" class="headerlink" title="开始打包"></a>开始打包</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> my_project_dir</div><div class="line"><span class="comment"># -w 参数指定打包为一个文件(.app)，-n 设置应用名，-i 设置应用图标。</span></div><div class="line">pyi-makespec -w -n MyAppName -i appicon.icns MyMainScript.py</div><div class="line"><span class="comment"># 修改.spec文件中的 datas 以添加数据文件</span></div><div class="line">pyinstaller MyAppName.spec</div><div class="line"><span class="comment"># 生成 built 文件夹和 dist 文件夹，打包好的文件在 dist 中。</span></div></pre></td></tr></table></figure><h2 id="附：使用PyQt过程中遇到的问题"><a href="#附：使用PyQt过程中遇到的问题" class="headerlink" title="附：使用PyQt过程中遇到的问题"></a>附：使用PyQt过程中遇到的问题</h2><h3 id="QSound不能播放音乐"><a href="#QSound不能播放音乐" class="headerlink" title="QSound不能播放音乐"></a>QSound不能播放音乐</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">QSoundEffect(qaudio): Error decoding source</div></pre></td></tr></table></figure><p>QSound只能播放<code>.wav</code></p><h3 id="QMediaPlayer没有声音"><a href="#QMediaPlayer没有声音" class="headerlink" title="QMediaPlayer没有声音"></a>QMediaPlayer没有声音</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">no error but no sound</div></pre></td></tr></table></figure><p>QUrl.fromLocalFile只能使用绝对路径</p><h3 id="QMediaPlayer-duration数据错误"><a href="#QMediaPlayer-duration数据错误" class="headerlink" title="QMediaPlayer.duration数据错误"></a>QMediaPlayer.duration数据错误</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">QMediaPlayer.duration() == <span class="number">0</span></div></pre></td></tr></table></figure><p><code>QMediaPlayer.setMedia()</code>是异步执行的，如果在这个方法后马上调用<code>QMediaPlayer.duration()</code>得到的将是错误的值，因为<code>QMediaPlayer.setMedia()</code>还未设置好。应该在<code>durationchanged</code>信号发出后重新给其赋值。</p><h3 id="QMediaPlaylist循环播放"><a href="#QMediaPlaylist循环播放" class="headerlink" title="QMediaPlaylist循环播放"></a>QMediaPlaylist循环播放</h3><p>设置只播一首歌时<code>setPlaybackMode(CurrentItemOnce)</code>，每次本首歌曲播完index会被重置为<code>-1</code>。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;最近的实验需要做一个桌面小程序来收集实验信息，决定选用跨平台的&lt;code&gt;Python&lt;/code&gt;来写。&lt;br&gt;在&lt;code&gt;Python图形界面编程&lt;/code&gt;中常用的工具包有：&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyGUI" scheme="http://huisblog.cn/tags/PyGUI/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统(Recommender Systems)</title>
    <link href="http://huisblog.cn/2017/09/11/recsys/"/>
    <id>http://huisblog.cn/2017/09/11/recsys/</id>
    <published>2017-09-11T13:18:14.000Z</published>
    <updated>2017-09-11T13:22:05.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>本文使用电影评分推荐的例子来解释两种推荐系统的工作原理，电影推荐的问题可以描述为：<br>已知一些用户对一些电影的评分，预测用户对未评价电影的评分。</p><table><thead><tr><th></th><th>用户1</th><th>用户2</th><th>用户3</th><th>用户4</th></tr></thead><tbody><tr><td>电影1</td><td>5</td><td>5</td><td>0</td><td>0</td></tr><tr><td>电影2</td><td>5</td><td>?</td><td>?</td><td>1</td></tr><tr><td>电影3</td><td>?</td><td>4</td><td>0</td><td>?</td></tr><tr><td>电影4</td><td>0</td><td>0</td><td>4</td><td>5</td></tr><tr><td>电影5</td><td>0</td><td>0</td><td>4</td><td>?</td></tr></tbody></table><a id="more"></a><p>即，已知：</p><ol><li>\(n_u=\) 用户数量；</li><li>\(n_m=\) 电影数量；</li><li>\(r(i,j)=1\) 如果用户\(j\)给电影\(i\)打了分，否则为0；</li><li>\(y^{(i,j)}=\) 用户\(j\)给电影\(i\)打的分数（只有当\(r(i,j)=1\)时才有定义）；</li><li>\(m^{(j)}=\) 用户\(j\)打分了的电影数量；</li></ol><p>估计：</p><ol><li>\(\theta^{(j)}=\) 用户\(j\)的参数向量；</li><li>\(x^{(i)}=\) 电影\(i\)的特征向量；</li><li>对于用户\(j\)，电影\(i\)，预测\((\theta^{(j)})^T(x^{(i)})\)</li></ol><h2 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h2><p>根据已知每个电影的特征\(x^{(i)}\)，学习用户偏好\(\theta^{(1)},…,\theta^{(n_u)}\)，则损失函数为：</p><p>$$min_{\theta^{(1)},…,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$</p><p>梯度下降：</p><p>$$\theta_k^{(j)} := \theta_k^{(j)} - \alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x^{(i)}\quad (for\quad k=0 )\ \theta_k^{(j)} := \theta_k^{(j)} - \alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x^{(i)}+\lambda\theta_k^{(j)}\quad)\quad (for\quad k\neq0 )$$</p><h2 id="使用协同过滤的推荐系统"><a href="#使用协同过滤的推荐系统" class="headerlink" title="使用协同过滤的推荐系统"></a>使用协同过滤的推荐系统</h2><p>若我们事先知道各用户的偏好参数 \(\theta^{(j)}\)，则可以学习电影的特征：</p><p>$$min_{x^{(1)},…,x^{(n_m)}}\frac{1}{2}\sum_{i=1}^{n_m}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2$$</p><p>那么：<br>给出\(x^{(1)},…,x^{(n_m)}\)和电影评分，可以估计\(\theta^{(1)},…,\theta^{(n_u)}\)；<br>给出\(\theta^{(1)},…,\theta^{(n_u)}\)和电影评分，可以估计\(x^{(1)},…,x^{(n_m)}\)。</p><p>这似乎是一个“先有鸡还是先有蛋”的问题，其实我们在解决这个问题时，可以先随机生成 \(\theta\)，然后估计\(x\)，接着根据\(x\)再次估计一组新的\(\theta\)，如此循环最后可以收敛到一组足够合理的结果。</p><p>$$ Guess\quad\theta \to x\to\theta\to x\to …$$</p><p>以上就是协同过滤的思想。但为了提高计算效率，实际上\(x^{(1)},…,x^{(n_m)}\)和\(\theta^{(1)},…,\theta^{(n_u)}\)是同时考虑的，这样可以将损失函数定义为：</p><p>$$J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2\+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$</p><p>协同过滤算法：</p><ol><li>使用小的随机值初始化\(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}\)；</li><li>使用梯度下降最小化损失函数\(J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})\)；</li><li>对偏好参数为\(\theta\)的用户和特征值为（学习到的）\(x\)的电影，预测其打分\(\theta^Tx\)</li></ol><blockquote><p>此时不再需要截距项：<br>\(x^{(i)}\in \mathbb{R}^n\)<br>\(\theta^{(j)}\in \mathbb{R}^n\)  </p></blockquote><p>协同过滤算法的向量化实现即低秩矩阵分解(Low Rank Matrix Factorization)的过程。</p><h3 id="Mean-Normalization"><a href="#Mean-Normalization" class="headerlink" title="Mean Normalization"></a>Mean Normalization</h3><p>在使用协同过滤算法时应该将打分矩阵均值正规化。因为对于无法得知偏好的新用户(初始化\(\theta^T=[0,0,…]\))，均值正规化可以避免\(\theta\)和\(x\)的最优解都是零矩阵（最后的结果其实就是把其他已知打分的均值赋给了新用户）。</p><h3 id="寻找相关电影"><a href="#寻找相关电影" class="headerlink" title="寻找相关电影"></a>寻找相关电影</h3><p>可以根据电影相似性进行推荐：比如对于喜欢电影\(i\)的用户，我们在学习其它电影的特征后将与电影\(i\)最相似的5个电影就是5个最小\(||x^{(i)}-x^{(j)}||\)的电影推荐给这个用户。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;问题定义&quot;&gt;&lt;a href=&quot;#问题定义&quot; class=&quot;headerlink&quot; title=&quot;问题定义&quot;&gt;&lt;/a&gt;问题定义&lt;/h2&gt;&lt;p&gt;本文使用电影评分推荐的例子来解释两种推荐系统的工作原理，电影推荐的问题可以描述为：&lt;br&gt;已知一些用户对一些电影的评分，预测用户对未评价电影的评分。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;用户1&lt;/th&gt;
&lt;th&gt;用户2&lt;/th&gt;
&lt;th&gt;用户3&lt;/th&gt;
&lt;th&gt;用户4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;电影1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;电影2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;电影3&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;电影4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;电影5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>异常检测(Anomaly Detection)</title>
    <link href="http://huisblog.cn/2017/09/11/anodet/"/>
    <id>http://huisblog.cn/2017/09/11/anodet/</id>
    <published>2017-09-11T13:14:58.000Z</published>
    <updated>2017-09-11T13:16:58.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="使用高斯分布探测异常值"><a href="#使用高斯分布探测异常值" class="headerlink" title="使用高斯分布探测异常值"></a>使用高斯分布探测异常值</h2><p>假设异常特征之间相互独立且\(x_i \sim N(\mu_i,\sigma_i^2)\)，则\(p(x)=\prod{p(x_j;\mu_j,\sigma_j^2)}\)</p><ol><li>选择标识异常的特征\(x_i\);</li><li>学习参数\(\mu_1,…,\mu_n, \sigma_1^2,…,\sigma_n^2\)</li><li>对于测试点\(x\)，计算\(p(x)\)，若\(p(x)&lt;\epsilon\)则报异常</li></ol><p>可使用混淆矩阵评估效果，并使用交叉验证集来选择\(\epsilon\)。</p><a id="more"></a><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>一般在选定特征之前会使用直方图描绘一下数据特征，看其是否符合高斯分布。<br>对于非高斯分布的特征而言，按照经验来说，可以尝试\(log(x+b)\)或者\(x^{1/b}\)是否满足要求。</p><p>在理想情况下，我们希望对于正常点来说\(p(x)\)足够大，而对于异常点来说\(p(x)\)足够小；<br>但是更普遍的情况是\(p(x)\)在正常点和异常点上都很大。在这些情况下我们就要考虑添加更加恰当的特征。<br>例如在机器状态监控中，选用的异常特征有：  </p><ol><li>\(x_1=\) 内存占用</li><li>\(x_2=\) 磁盘访问</li><li>\(x_3=\) CPU负载</li><li>\(x_4=\) 网络流量</li></ol><p>但\(x_3, x_4\)在异常情况和非异常情况下都有可能极大或者极小，因此可以考虑添加特征：<br>$$x_5=\frac{CPU负载}{网络流量},\quad x_6=\frac{(CPU负载)^2}{网络流量}$$</p><h2 id="异常检测与监督学习"><a href="#异常检测与监督学习" class="headerlink" title="异常检测与监督学习"></a>异常检测与监督学习</h2><table><thead><tr><th>异常检测</th><th>监督学习</th></tr></thead><tbody><tr><td>正例数量非常少(一般0-20)</td><td>非常多的正例和负例</td></tr><tr><td>负例数量非常多</td><td></td></tr><tr><td>异常值很“多样”，很难根据现有正例数据学习异常特征</td><td>有足够的正例以供学习其特征，将来出现的正例很可能与训练集中已有的某些相似</td></tr><tr><td>将来出现的异常可能和现有的异常值完全不同</td><td></td></tr><tr><td>举例：</td><td></td></tr><tr><td>诈骗检测</td><td>垃圾邮件分类</td></tr><tr><td>制造业 (比如飞机引擎)</td><td>天气预报 (晴/雨/其它)</td></tr><tr><td>数据中心的机器监控</td><td>癌症检测</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;使用高斯分布探测异常值&quot;&gt;&lt;a href=&quot;#使用高斯分布探测异常值&quot; class=&quot;headerlink&quot; title=&quot;使用高斯分布探测异常值&quot;&gt;&lt;/a&gt;使用高斯分布探测异常值&lt;/h2&gt;&lt;p&gt;假设异常特征之间相互独立且\(x_i \sim N(\mu_i,\sigma_i^2)\)，则\(p(x)=\prod{p(x_j;\mu_j,\sigma_j^2)}\)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择标识异常的特征\(x_i\);&lt;/li&gt;
&lt;li&gt;学习参数\(\mu_1,…,\mu_n, \sigma_1^2,…,\sigma_n^2\)&lt;/li&gt;
&lt;li&gt;对于测试点\(x\)，计算\(p(x)\)，若\(p(x)&amp;lt;\epsilon\)则报异常&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可使用混淆矩阵评估效果，并使用交叉验证集来选择\(\epsilon\)。&lt;/p&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>K-means和PCA</title>
    <link href="http://huisblog.cn/2017/07/04/ng_cluster/"/>
    <id>http://huisblog.cn/2017/07/04/ng_cluster/</id>
    <published>2017-07-04T08:45:02.000Z</published>
    <updated>2017-07-04T08:59:56.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p><code>K-means</code>是一个常用且有效的聚类算法。它的算法可以描述为：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Firstly randomly initiallize the K centroids mu1,mu2,...,muk;</div><div class="line">Repeat&#123;</div><div class="line">% Cluster assignment step</div><div class="line">for i = <span class="number">1</span> to m</div><div class="line">ci = index of cluster centroid cloest to xi;</div><div class="line">% Move centroid</div><div class="line">for k = <span class="number">1</span> to k</div><div class="line">muk = mean of points xs assigned to cluster k;</div><div class="line">&#125;</div></pre></td></tr></table></figure><a id="more"></a><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>像逻辑回归等监督学习算法一样，<code>K-means</code>等无监督学习算法同样有损失函数来作为优化目标：</p><p>$$min_{c,\mu}J(c^{(1)},…,c^{(m)},\mu_1,…,.\mu_K) = \frac{1}{m}||x^{(i)}-\mu_{c^{(i)}}||^2$$</p><p>\(minJ\)表示我们的优化目标是最小化同类点到聚类中心的距离。</p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>当\(K\)值较小的时候，初始中心点的位置对最后的聚类结果有非常大的影响。只随机生成一组初始点可能会导致最后的结果是局部最优解<code>local optima</code>。因此，当\(K\)值较小的时候，我们可以使用多组初始中心点以避免这个问题：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> to <span class="number">100</span> &#123;</div><div class="line">Randomly initialize K-means;</div><div class="line">Run K-means, get c1,...,cm, mu1,...,muk;</div><div class="line">Compute cost function/distortion; </div><div class="line">&#125;</div><div class="line"></div><div class="line">Choose the cluster that gave the lowest cost J.</div></pre></td></tr></table></figure><h3 id="选择聚类数目"><a href="#选择聚类数目" class="headerlink" title="选择聚类数目"></a>选择聚类数目</h3><p>怎样选择聚类数目\(K\)是一个很难说的问题，有时就算是人类专家在看到一组可视化数据后都很难给出精准的答案。现有的选择\(K\)的算法有<code>ELbow Method</code>，但它对很多情况都并不适用。通常来说，决定\(K\)的大小需要我们根据具体问题和之后的目标来具体分析。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ol><li>市场分割</li><li>社交网络分析</li><li>计算集群组织</li><li>天文数据分析</li></ol><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p><code>PCA</code>也是最常用的无监督算法之一，它的算法可以描述为：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.</span> Feature Preprocessing: feature scaling and mean normaliza1on;</div><div class="line"><span class="number">2.</span> Compute covariance matrix:</div><div class="line">Sigma = <span class="number">1</span>/m * X' * X;</div><div class="line"><span class="number">3.</span> Compute eigenvectors of matrix Sigma:</div><div class="line">[U,S,V] = svd(Sigma);</div><div class="line"><span class="number">4.</span> Ureduce = U(:,<span class="number">1</span>:k);</div><div class="line"><span class="number">5.</span> Xreduce = X * Ureduce;</div><div class="line"><span class="number">6.</span> Xrecover = Xreduce * Ureduce';</div></pre></td></tr></table></figure><h3 id="优化目标-1"><a href="#优化目标-1" class="headerlink" title="优化目标"></a>优化目标</h3><p>PCA的目标可以描述为：要找到最合适的一组向量\(\mu^{(1)},…,\mu^{(k)}\)，将原始数据映射到这组向量围成的新的特征空间中，使其<code>投影距离最小(minimize the projec1on error)</code>。</p><h3 id="选择主成分数量"><a href="#选择主成分数量" class="headerlink" title="选择主成分数量"></a>选择主成分数量</h3><p>通常，我们可以用到达一定<code>保留方差(retained variance)</code>的最小\(k\)值来作为主成分数量。可以看一个保留了99%方差的例子：</p><p>$$\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x^{(1)}_{approx}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2} \le 0.01 $$</p><p>上述值可以通过协方差矩阵奇异值分解后得到的\(S\)来计算，\(S\)是一个对角矩阵，上述不等式等同于：</p><p>$$\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^mS_{ii}}\ge 0.99$$</p><h3 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h3><p>应该用于：</p><ol><li>数据压缩：减小数据储存代价、加快运算速度（\(k&lt;n\)，由保留方差计算）；</li><li>数据可视化（\(k=2 \quad or\quad k=3\)）</li></ol><p>错误的应用：</p><ol><li>减小特征集以防止过拟合：PCA只分析了特征之间\(x_1,…x_n\)的关系，并没有考虑特征的变化对\(y\)的影响，使用PCA可能会丢失和\(y\)相关的重要信息。因此防止过拟合不应该用PCA，而应该使用正则化。</li><li>在机器学习系统的开始设计阶段使用：应该先使用原始数据，只有在原始数据效果不好（太慢之类的）时候再考虑用PCA。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;K-means&quot;&gt;&lt;a href=&quot;#K-means&quot; class=&quot;headerlink&quot; title=&quot;K-means&quot;&gt;&lt;/a&gt;K-means&lt;/h2&gt;&lt;p&gt;&lt;code&gt;K-means&lt;/code&gt;是一个常用且有效的聚类算法。它的算法可以描述为：&lt;/p&gt;
&lt;figure class=&quot;highlight matlab&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Firstly randomly initiallize the K centroids mu1,mu2,...,muk;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Repeat&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	% Cluster assignment step&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	for i = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; to m&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		ci = index of cluster centroid cloest to xi;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	% Move centroid&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	for k = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; to k&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		muk = mean of points xs assigned to cluster k;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机(SVM)</title>
    <link href="http://huisblog.cn/2017/06/26/svm/"/>
    <id>http://huisblog.cn/2017/06/26/svm/</id>
    <published>2017-06-26T13:10:58.000Z</published>
    <updated>2017-06-26T13:18:17.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="支持向量机的假设函数和损失函数"><a href="#支持向量机的假设函数和损失函数" class="headerlink" title="支持向量机的假设函数和损失函数"></a>支持向量机的假设函数和损失函数</h2><p>回忆一下逻辑回归的损失函数：</p><p>$$min_\theta\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}(-logh_\theta(x^{(i)})+(1-y^{(i)})(-log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$</p><p>支持向量机的损失函数与之类似：</p><p>$$min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$$</p><a id="more"></a><p>\(cost_{1}(z) \quad (y=1)\)：</p><img src="/2017/06/26/svm/cost1.png"><p>\(cost_{0}(z) \quad (y=0)\)：</p><img src="/2017/06/26/svm/cost0.png"><p>注意到除了从\(-log((1-)\frac{1}{1+e^z})\)到\(cost_{0/1}(z)\)的变化外，用于<code>trade-off</code>的\(\lambda\)也在支持向量机中通常以参数\(C\)的形式出现，\(C\)可以理解为\(\frac{1}{\lambda}\)。</p><p>支持向量机的假设函数：</p><p>$$\begin{equation}<br>h_\theta(x)=\begin{cases}<br>1 &amp; \text{if}\quad \theta^Tx\ge 0 \\<br>0 &amp; \text{otherwise}<br>\end{cases}<br>\end{equation}$$</p><h2 id="Large-Margin-Classification"><a href="#Large-Margin-Classification" class="headerlink" title="Large Margin Classification"></a>Large Margin Classification</h2><p>SVM常常被称作为<code>Large Margin Classification</code>，为什么呢？因为它对于决策边界要求更加严格：</p><ul><li>如果\(y=1\)，我们想要\(\theta^Tx \ge 1\)（而不是\(\ge 0\)）；</li><li>如果\(y=0\)，我们想要\(\theta^Tx \le -1\)（而不是\(&lt; 0\)）；</li></ul><p>当满足以上条件时，\(cost_{0/1}(z)=0\)，从而将\(minJ(\theta)\)转化为以下的凸优化问题：</p><p>$$\begin{align}&amp; min_\theta\frac{1}{2}\sum_{j=1}^{n}\theta_j^2 \ <br>&amp; s.t. \begin{cases}<br>\theta^Tx^{(i)}\ge 1 &amp; \text{if} \quad y^{(i)}=1 \\<br>\theta^Tx^{(i)}\le -1 &amp; \text{if} \quad y^{(i)}=0<br>\end{cases}<br>\end{align}$$</p><p>将\(x^{(i)}\)在向量\(\theta\)上的投影写作\(p^{(i)}\)，可以变化形式为：</p><p>$$\begin{align}&amp; min_\theta\frac{1}{2}\sum_{j=1}^{n}\theta_j^2 \ <br>&amp; s.t. \begin{cases}<br>p^{(i)}||\theta||\ge 1 &amp; \text{if} \quad y^{(i)}=1 \\<br>p^{(i)}||\theta||\le -1 &amp; \text{if} \quad y^{(i)}=0<br>\end{cases}<br>\end{align}$$</p><p>\(\theta\)向量是决策边界\(\theta^Tx=0\)的法向量，为了使\(||\theta||\)（即目标函数）尽量小，在条件限制下，\(p\)就要尽可能大，即\(x\)在\(\theta\)方向的投影尽可能大，这就使得决策边界离两边数据尽可能远。</p><p>SVM是凸函数优化，最后的结果是全局最优点，不需要担心局部最优问题。</p><h2 id="核函数-Kernels"><a href="#核函数-Kernels" class="headerlink" title="核函数(Kernels)"></a>核函数(Kernels)</h2><p>核函数将一系列特征\(x_1,…x_n\)映射到另一组特征\(f_1,…\)上，可以给SVM加上非线性性质。它有两个重要概念：</p><ol><li><code>地标点(landmarks)</code>：\(l^{(1)},…,l^{(n)}\)；</li><li><code>近似度(Similarity)</code>:<br>$$similarity(x,l)=1\quad \text{if}\quad x\approx l\ \\similarity(x,l)=0\quad \text{if (x) is far away form }l$$</li></ol><p>地标点一般直接使用训练集中的点\(l^{(1)} = x^{(1)},…,l^{(m)} = x^{(m)}\)；<br>常用的近似度函数有:</p><ul><li><code>线性核函数(linear kernel)</code>：线性核函数其实就是不用核函数，\(f_n = x_n\)；</li><li><code>高斯核函数(Gaussian kernel)</code>：\(f_m = similarity(x,l)=exp(-\frac{||x-l^{(m)}||}{2\sigma^2})\)</li></ul><p>在高斯核函数中，\(\sigma\)也可以起到<code>trade-off</code>的作用：</p><ul><li>\(\sigma\)过小：高方差、低偏差；</li><li>\(\sigma\)过大：高偏差、低方差。</li></ul><h2 id="使用SVM"><a href="#使用SVM" class="headerlink" title="使用SVM"></a>使用SVM</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">1. First choose parameter C and kernel;</div><div class="line">2. Given training data: (x1,y1),(x2,y2)...(xm,ym), choose l1=x1,..., lm=xm;</div><div class="line">3. mapping features using similarity:</div><div class="line">f1 = similarity(x,l1)</div><div class="line">.</div><div class="line">.</div><div class="line">.</div><div class="line">fm = similarity(x,lm)</div><div class="line"></div><div class="line">4. Feature scaling</div><div class="line">5. traing svm with f, C, sigma(hypotyhesis: predict y=1 if theta*f &gt;= 0)</div></pre></td></tr></table></figure><h2 id="逻辑回归和SVM"><a href="#逻辑回归和SVM" class="headerlink" title="逻辑回归和SVM"></a>逻辑回归和SVM</h2><ol><li>\(n\)很大（和\(m\)相近）：使用逻辑回归或者不带核函数的（或者线性核函数）的SVM，这两者效果类似，不用复杂的非线性特征以防止过拟合；</li><li>\(n\)很小，\(m\)不大不小 (intermediate) ：使用带高斯核的SVM；</li><li>\(n\)很小，\(m\)很大：增加特征，然后使用逻辑回归或者不带核函数的SVM。</li></ol><p>神经网络在以上三种情况下都表现很好，但是训练速度很慢。</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;支持向量机的假设函数和损失函数&quot;&gt;&lt;a href=&quot;#支持向量机的假设函数和损失函数&quot; class=&quot;headerlink&quot; title=&quot;支持向量机的假设函数和损失函数&quot;&gt;&lt;/a&gt;支持向量机的假设函数和损失函数&lt;/h2&gt;&lt;p&gt;回忆一下逻辑回归的损失函数：&lt;/p&gt;
&lt;p&gt;$$min_\theta\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}(-logh_\theta(x^{(i)})+(1-y^{(i)})(-log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$$&lt;/p&gt;
&lt;p&gt;支持向量机的损失函数与之类似：&lt;/p&gt;
&lt;p&gt;$$min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^n\theta_j^2$$&lt;/p&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>提升学习算法性能</title>
    <link href="http://huisblog.cn/2017/06/20/betterMLA/"/>
    <id>http://huisblog.cn/2017/06/20/betterMLA/</id>
    <published>2017-06-20T13:54:51.000Z</published>
    <updated>2017-06-20T13:57:41.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p>在解决一个机器学习问题时，如果算法效果不理想，要怎么办呢？直觉上，我们可以有一些解决方法：</p><ul><li>增加训练数据  </li><li>减小特征集  </li><li>增加特征  </li><li>增加多项式特征</li><li>减小\(\lambda\)  </li><li>变大\(\lambda\)  </li><li>…</li></ul><p>然而，这些方法并不是在所有情况下都适用的，有时候，花很多时间增加训练数据也许根本不能使分类结果变得更好。因此，我们需要使用一些简单的技巧来选择使用哪些方法，这样的技巧被称为<code>机器学习诊断(machine learning diagnostics)</code>。</p><a id="more"></a><h2 id="评估假设函数"><a href="#评估假设函数" class="headerlink" title="评估假设函数"></a>评估假设函数</h2><p>在解决一个机器学习问题时，我们往往会选择不同的模型来训练学习算法，然后评估比较其效果。那么怎样的评估是正确有效的呢？一个常用的方法是，将数据集分为三部分：  </p><ol><li>训练集(60%)：使用训练集计算每一个模型的优化参数\(\Theta\)；</li><li>交叉验证集(20%)：使用验证集计算每个模型的误差，选出误差最小的模型；</li><li>测试集(20%)：使用测试集计算误差最小模型的<code>通用误差(generalization error)</code>。</li></ol><p>注意：设置交叉验证集的目的是将验证集误差与测试集误差区分开来，使得测试集误差可以作为通用误差的评估依据。我们在选择模型时针对验证集做了优化（因为选的是最小误差的验证集），因此验证集误差是会小于测试集误差的。</p><h2 id="机器学习诊断"><a href="#机器学习诊断" class="headerlink" title="机器学习诊断"></a>机器学习诊断</h2><h3 id="诊断高偏差和高方差"><a href="#诊断高偏差和高方差" class="headerlink" title="诊断高偏差和高方差"></a>诊断高偏差和高方差</h3><ul><li>高偏差（欠拟合）：\(J_{train}(\Theta)\)和\(J_{CV}(\Theta)\)都会很大，且\(J_{CV}(\Theta) \approx J_{train}(\Theta)\)；</li><li>高方差（过拟合）：：\(J_{train}(\Theta)\)会很小，\(J_{CV}(\Theta)\)会远大于\(J_{train}(\Theta)\)。</li></ul><p>例如下图：</p><img src="/2017/06/20/betterMLA/BV.png"><p>正则化系数\(\lambda\)选择不合理也可能导致欠拟合或者过拟合，因此\(\lambda\)可以和不同模型组合在一起进行实验选择：</p><ol><li>创建一组\(\lambda\)；</li><li>创建一组假设函数；</li><li>迭代学习每个\(\lambda\)与假设函数组合的\(\Theta\)；</li><li>对每一个\(\Theta\)计算验证集上的错误率（计算错误率不加入正则项）；</li><li>选择误差最小的组合；</li><li>计算测试集误差来评估它是否有好的通用性。</li></ol><h3 id="学习曲线-Learning-Curves"><a href="#学习曲线-Learning-Curves" class="headerlink" title="学习曲线(Learning Curves)"></a>学习曲线(Learning Curves)</h3><p>学习曲线是错误率随训练集大小的变化而变化的曲线，可以用来诊断欠拟合和过拟合问题。</p><h4 id="在高偏差的时候"><a href="#在高偏差的时候" class="headerlink" title="在高偏差的时候"></a>在高偏差的时候</h4><ul><li>训练集小：\(J_{train}(\Theta)\)小，\(J_{CV}(\Theta)\)大；</li><li>训练集大：\(J_{train}(\Theta)\)和\(J_{CV}(\Theta)\)都很大，且\(J_{CV}(\Theta) \approx J_{train}(\Theta)\)。</li></ul><p>当出现高偏差的时候，加大训练集并不能带来多大的帮助：</p><img src="/2017/06/20/betterMLA/highB.png"><h4 id="在高方差的时候"><a href="#在高方差的时候" class="headerlink" title="在高方差的时候"></a>在高方差的时候</h4><ul><li>训练集小：\(J_{train}(\Theta)\)小，\(J_{CV}(\Theta)\)大；</li><li>训练集大：\(J_{train}(\Theta)\)随着训练集增大而变大，\(J_{CV}(\Theta)\)随着训练增大而变小，但不会趋于平缓，且\(J_{CV}(\Theta) &lt; J_{train}(\Theta)\)并差距依旧明显。</li></ul><p>当出现高方差的时候，加大训练集可能会提高性能：</p><img src="/2017/06/20/betterMLA/highV.png"><h3 id="处理方法小结"><a href="#处理方法小结" class="headerlink" title="处理方法小结"></a>处理方法小结</h3><ul><li>增加训练数据  &lt;– 高方差</li><li>减小特征集  &lt;– 高方差</li><li>增加特征  &lt;– 高偏差</li><li>增加多项式特征 &lt;– 高偏差</li><li>减小\(\lambda\)  &lt;– 高偏差</li><li>变大\(\lambda\)  &lt;– 高方差</li></ul><p>特别的，对于神经网络：  </p><ul><li>较低阶的多项式（模型复杂度低）可能导致高偏差和低方差。在这种情况下，模型拟合结果差；</li><li>较高阶的多项式（模型复杂度高）拟合训练集非常好，拟合测试集非常不好，导致训练数据上的低偏差和高方差。</li><li>实际上，我们希望达到两者之间的状态，即在拟合数据好的情况下也保证通用性好。</li></ul><h2 id="一般化流程"><a href="#一般化流程" class="headerlink" title="一般化流程"></a>一般化流程</h2><p>一般来说，解决机器学习问题的推荐方法是：</p><ol><li>从一个简单的算法开始，快速实现，尽早在验证集上测试看一下效果；</li><li>画出学习曲线来决定是否要增加数据、特征等；</li><li>进行实验，计算验证集误差，尝试找出大部分错误的来源趋势。</li></ol><p>在选择使用的特征和模型时，始终要考虑两个问题：<br>Q1. 如果是人类专家来根据这些给出的特征进行预测，Ta能不能做出有效判断？即，使用的特征是否含有足够的有效信息；<br>Q2. 使用的模型是否具有足够的复杂度来解决这个问题？多参数的复杂算法和大的训练数据集相结合可以有效解决高偏差问题，同时不至于带来高方差。</p><h2 id="偏态数据-skewed-data"><a href="#偏态数据-skewed-data" class="headerlink" title="偏态数据(skewed data)"></a>偏态数据(skewed data)</h2><p>当训练数据中正负例数量非常不均衡时，比如正例远远少于负例（检测癌症患者的实验中，患病者的数目远远少于未患病者的数目），这样的数据就叫做<code>偏态数据(skewed data)</code>。</p><p>在遇到偏态数据时，只使用<code>正确率(accuracy)</code>作为<code>误差评估指标(error metrics)</code>是不合适的，因为这可能会导致明显错误的选择。比如，在诊断是否患该症的问题中始终得到不患病的结果可以得到很高的正确率，但这样的诊断方式是显然不对的。</p><p>所以，需要有新的指标来衡量算法效果。常用的有<code>精确度(Precision)</code>和<code>查全率(Recall)</code>。</p><p>定义混淆矩阵如下：</p><table><thead><tr><th></th><th>实际值1</th><th>实际值0</th></tr></thead><tbody><tr><td>分类值1</td><td>Ture Positive</td><td>False Positive</td></tr><tr><td>分类值0</td><td>False Negative</td><td>Ture Negative</td></tr></tbody></table><p>$$\begin{align} Precision = \frac{True Positive}{Ture Positive + False Positive} \newline \newline Recall = \frac{True Positive}{Ture Positive + False Negative}\end{align}$$</p><p>使用精确度和查全率可以有效消除单独使用正确率带来的偏态数据处理不当问题。</p><p>但是，这样带来的另一个问题是，有了多个衡量性能的标准。而我们知道，有一个单独的实数作为<code>误差评估指标</code>(A single real number evaluation metric)是非常重要且必要的。同时，为了权衡精确度和查全率（设置分类阈值），可以使用<code>F1 Score</code>来作为误差评估指标：</p><p>$$\begin{align} F_1 Score = 2\frac{Precision\times Recall}{Precision+ Recall}\end{align}$$</p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;在解决一个机器学习问题时，如果算法效果不理想，要怎么办呢？直觉上，我们可以有一些解决方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加训练数据  &lt;/li&gt;
&lt;li&gt;减小特征集  &lt;/li&gt;
&lt;li&gt;增加特征  &lt;/li&gt;
&lt;li&gt;增加多项式特征&lt;/li&gt;
&lt;li&gt;减小\(\lambda\)  &lt;/li&gt;
&lt;li&gt;变大\(\lambda\)  &lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而，这些方法并不是在所有情况下都适用的，有时候，花很多时间增加训练数据也许根本不能使分类结果变得更好。因此，我们需要使用一些简单的技巧来选择使用哪些方法，这样的技巧被称为&lt;code&gt;机器学习诊断(machine learning diagnostics)&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>神经网络(Neural Networks)</title>
    <link href="http://huisblog.cn/2017/06/15/neuralnetwork/"/>
    <id>http://huisblog.cn/2017/06/15/neuralnetwork/</id>
    <published>2017-06-15T08:54:11.000Z</published>
    <updated>2017-06-15T15:38:51.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="Why-Neural-Networks"><a href="#Why-Neural-Networks" class="headerlink" title="Why Neural Networks?"></a>Why Neural Networks?</h2><p>当遇到<code>特征数量很大(n is large)</code>的<code>复杂非线性问题</code>时，简单的逻辑回归（加入二次项或三次项）不再适用，因为会导致太多的特征量参与计算（不仅仅是\(x_n, x_n^2,x_n^3\)，还有\(x_i{x_j},x_i^2{x_j}\)等二次项和三次项）—— 带来巨大的计算代价和过拟合问题。</p><p>神经网络其实很早就出现了(A pretty old algorithm)，始于80年代，90年代有所衰减而现在又火了起来（计算机计算能力的提升）。神经网络模拟人脑神经的计算机制，在解决许多机器学习问题时有很好的效果。</p><p>人脑使用一种学习算法来处理无数不同的问题，这使得我们模拟它的计算机制成为可能。人脑甚至可以学习任何传感数据，这让神经网络成为最有可能实现人工智能的方法。</p><a id="more"></a><h2 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h2><p>最简单的神经网络表达式：</p><p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}   \newline \end{bmatrix}\rightarrow h_\theta(x)$$</p><p>这是一个只有2层的神经网络，第1层<code>输入层</code>\(\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\)和第2层<code>输出层</code>\(h_\theta(x)\)。</p><p>输入层和输出层之间可以有中间层节点，称为<code>隐藏层(hidden layers)</code>。有一个隐藏层的神经网络可以表示为：</p><p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)$$</p><p>$$\begin{align}&amp; a_i^{(j)} = \text{“activation” of unit i in layer j} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer j to layer j+1}\end{align}$$</p><p>每个激活节点的计算方式：</p><p>$$\begin{align} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align}$$</p><p>值得注意的是，如果神经网络在第\(j\)层有\(s_j\)个节点，在第\(j+1\)层有\(s_{j+1}\)个节点，那么\(\Theta^{(j)}\)的维度会是\(s_{j+1}\times(s_j +1)\)。</p><p>增加隐藏层可以解决更多的复杂非线性问题。</p><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><h3 id="二元逻辑运算"><a href="#二元逻辑运算" class="headerlink" title="二元逻辑运算"></a>二元逻辑运算</h3><p>神经网络可以实现所有的逻辑门，例如：</p><p>$$\begin{align}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align}$$</p><h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p>比如将数据分为4类，表达式可以是：</p><p>$$\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline … \newline x_n\end{bmatrix}\rightarrow\begin{bmatrix}a_0^{(2)} \newline a_1^{(2)} \newline a_2^{(2)} \newline … \newline \end{bmatrix}\rightarrow\begin{bmatrix}a_0^{(3)} \newline a_1^{(3)} \newline a_2^{(3)} \newline …\newline \end{bmatrix}\rightarrow … \rightarrow \begin{bmatrix}h_\Theta(x)_1 \newline h_\Theta(x)_2 \newline h_\Theta(x)_3 \newline h_\Theta(x)_4\end{bmatrix}$$</p><p>分别训练4个假设函数，表示每个分类的可能性，取可能性最大的标签为分类结果。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>神经网络的损失函数标准形式：<br>$$\begin{gather} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather}$$</p><p>$$\begin{align}&amp; L = \text{ total number of layers in the network} \newline&amp; s_l = \text{number of units (not counting bias unit) in layer (l)}\newline&amp; K = \text{number of output units/classes}\end{align}$$</p><p>注意，第\(l\)层的\(\Theta^{(l)}\)矩阵大小为\(s_{l+1}\times (s_l + 1)\)，因此正则项\(\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\)是除去截距项(\(\Theta^{(l)}\)第一列)的所有\(\theta\)的平方和。</p><p>这个损失函数\(J(\Theta)\)是非凸的，因此使用梯度下降可能只能得到局部最优值。</p><h2 id="反向传播-Backprogation"><a href="#反向传播-Backprogation" class="headerlink" title="反向传播(Backprogation)"></a>反向传播(Backprogation)</h2><p><code>反向传播算法</code>是用于计算损失函数偏导\(\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)\)的算法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Given training set &#123;(x(1),y(1))⋯(x(m),y(m))&#125;, set Δ(l)i,j := 0 for all (l,i,j), (hence you end up having a matrix full of zeros)</div><div class="line"></div><div class="line">For training example t =1 to m:</div><div class="line">1. Set a(1) := x(t)</div><div class="line">2. Perform forward propagation to compute a(l) for l=2,3,…,L</div><div class="line">3. Using y(t), compute delta(L) = a(L)−y(t)</div><div class="line">4. Compute delta(L−1),delta(L−2),…,delta(2) using delta(l) = ((Θ(l))&apos;delta(l+1)) .∗ a(l) .∗ (1−a(l))</div><div class="line">5. Delta(l):= Delta(l)+ delta(l+1)(a(l))&apos;</div><div class="line">6. Regularization as the following equation.</div></pre></td></tr></table></figure><p>$$\begin{align}&amp; D_{i,j}^{(l)} = \frac{1}{m}(\Delta_{i,j}^{(l)} + \lambda \Theta_{i,j}^{(l)}) \quad \text{if (j \neq 0)}\newline&amp;  D_{i,j}^{(l)} = \frac{1}{m}\Delta_{i,j}^{(l)} \quad \text{if (j = 0)}\end{align}$$</p><p>最后得到的\(D_{i,j}^{(l)}\)即要求的偏导\(\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)\)。</p><p>可以使用<code>梯度检查(Gradient Checking)</code>来检查反向传播算法实现得是否正确。其依据：<br>$$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$$</p><p>对于每一个\(\Theta^{(j)}\)即：</p><p>$$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$$</p><p>当\(\epsilon = 10^{-4} \)时，可以保证其数学意义（足够小），再小的话计算机在计算的时候会出现数值错误，梯度检查的MATLAB实现：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">epsilon = <span class="number">1e-4</span>;</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</div><div class="line">  thetaPlus = theta;</div><div class="line">  thetaPlus(<span class="built_in">i</span>) += epsilon;</div><div class="line">  thetaMinus = theta;</div><div class="line">  thetaMinus(<span class="built_in">i</span>) -= epsilon;</div><div class="line">  gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*epsilon)</div><div class="line"><span class="keyword">end</span>;</div></pre></td></tr></table></figure><h2 id="神经网络的使用和训练"><a href="#神经网络的使用和训练" class="headerlink" title="神经网络的使用和训练"></a>神经网络的使用和训练</h2><h3 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h3><p>在准备使用神经网络来解决问题时，首先要设计神经网络的结构，包括有多少层隐藏层以及每层有多少节点：</p><ol><li>输入层节点数，即特征\(x^{(i)}\)的维度；</li><li>输出层节点数，即分类的类数；</li><li>隐藏层数目和每层的节点数，通常越多越好，但必须平衡随之增加的计算代价；</li><li>默认原则：1个隐藏层，如果有1个以上的隐藏层，那么建议每层的节点数一样。</li></ol><h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ol><li>随机生成每层的\(\Theta^{(l)}\)；</li><li>使用正向传播算出每个训练数据\(x^{(i)}\)的预测值\(h_{\Theta}(x^{(i)})\)；</li><li>实现损失函数；</li><li>使用反向传播算法实现损失函数偏导计算；</li><li>使用梯度检查确保反向传播算法正确实现；</li><li>使用梯度下降或其他优化算法来最小化损失函数，得到最终的\(\Theta^{(l)}\)值。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;Why-Neural-Networks&quot;&gt;&lt;a href=&quot;#Why-Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Why Neural Networks?&quot;&gt;&lt;/a&gt;Why Neural Networks?&lt;/h2&gt;&lt;p&gt;当遇到&lt;code&gt;特征数量很大(n is large)&lt;/code&gt;的&lt;code&gt;复杂非线性问题&lt;/code&gt;时，简单的逻辑回归（加入二次项或三次项）不再适用，因为会导致太多的特征量参与计算（不仅仅是\(x_n, x_n^2,x_n^3\)，还有\(x_i{x_j},x_i^2{x_j}\)等二次项和三次项）—— 带来巨大的计算代价和过拟合问题。&lt;/p&gt;
&lt;p&gt;神经网络其实很早就出现了(A pretty old algorithm)，始于80年代，90年代有所衰减而现在又火了起来（计算机计算能力的提升）。神经网络模拟人脑神经的计算机制，在解决许多机器学习问题时有很好的效果。&lt;/p&gt;
&lt;p&gt;人脑使用一种学习算法来处理无数不同的问题，这使得我们模拟它的计算机制成为可能。人脑甚至可以学习任何传感数据，这让神经网络成为最有可能实现人工智能的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归(Logistic Regression)</title>
    <link href="http://huisblog.cn/2017/06/03/logisticreg/"/>
    <id>http://huisblog.cn/2017/06/03/logisticreg/</id>
    <published>2017-06-03T07:11:54.000Z</published>
    <updated>2017-06-06T01:48:08.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h2 id="分类表达式"><a href="#分类表达式" class="headerlink" title="分类表达式"></a>分类表达式</h2><h3 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h3><ul><li>分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。  </li><li>分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；  </li><li>而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。</li></ul><h3 id="分类问题的假设表达式"><a href="#分类问题的假设表达式" class="headerlink" title="分类问题的假设表达式"></a>分类问题的假设表达式</h3><h4 id="逻辑函数-Logistic-Function-or-Sigmoid-Function"><a href="#逻辑函数-Logistic-Function-or-Sigmoid-Function" class="headerlink" title="逻辑函数(Logistic Function or Sigmoid Function)"></a>逻辑函数(Logistic Function or Sigmoid Function)</h4><p>使用逻辑函数\(g(z)\)将预测值映射到[0, 1]区间中，使假设函数更适用于分类问题。</p><p>$$\begin{align}&amp;<br>h_\theta (x) = g ( \theta^T x )<br>\newline<br>\newline&amp; z = \theta^T x<br>\newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}<br>\end{align}$$</p><a id="more"></a><p>逻辑函数和输入值：</p><img src="/2017/06/03/logisticreg/sigmoid.png"><p>可以注意到：</p><p>$$\begin{align}z=0, e^{0}=1 \Rightarrow g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align}$$</p><h4 id="假设函数的物理意义"><a href="#假设函数的物理意义" class="headerlink" title="假设函数的物理意义"></a>假设函数的物理意义</h4><p>\(h_{\theta}(x)\)表示预测值等于1的可能性：</p><p>$$\begin{align}&amp;<br>h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta)<br>\newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1<br>\end{align}$$</p><h3 id="判定边界-Decision-Boundary"><a href="#判定边界-Decision-Boundary" class="headerlink" title="判定边界(Decision Boundary)"></a>判定边界(Decision Boundary)</h3><p>判定边界是指分隔开不同类特征点的曲线（面），它由\(h_{\theta}(x)\)假设函数决定。<br>例如下面这个例子（二分类）中，判定边界是直线\(x_1=5\)</p><p>$$\begin{align}&amp; \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline &amp; y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline &amp; 5 - x_1 \geq 0 \newline &amp; - x_1 \geq -5 \newline&amp; x_1 \leq 5 \newline \end{align}$$</p><p>注意逻辑函数(\(e.g. \theta^Tx\))的输入不一定是线性的，它可以是任何适合训练数据的形状，比如圆(\(e.g. z =\theta_0+\theta_1x_1^2+\theta_2x_2^2\))。</p><h2 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h2><h3 id="损失函数和梯度下降"><a href="#损失函数和梯度下降" class="headerlink" title="损失函数和梯度下降"></a>损失函数和梯度下降</h3><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>不能直接使用之前线性回归中使用的损失函数\(J(\theta)=\frac{1}{2m}\sum_{i=0}^{i=m}(h_{\theta}(x_i)-y_i)^2\)，使用逻辑函数会导致其发生扭曲，产生许多局部最优点（极小值），换句话说就不再是凸函数了。<br>所以将损失函数修改为：</p><p>$$\begin{align}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align}$$</p><p>即：</p><p>$$J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$$</p><p>向量形式为：</p><p>$$\begin{align} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align}$$</p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>梯度下降的标准形式：</p><p>$$\begin{align}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align}$$</p><p>代入以上\(J(\theta)\)得到：</p><p>$$\begin{align} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align}$$</p><p>向量形式为：</p><p>$$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$$</p><h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p><code>Conjugate gradient</code>、 <code>BFGS</code>和<code>L-BFG</code>是梯度下降的优化算法，MATLAB有提供这几个库函数，使用这几个函数同样需要定义\(J(\theta)\)和\(\dfrac{\partial}{\partial \theta_j}J(\theta)\)。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></div><div class="line">  jVal = [...code to compute J(theta)...];</div><div class="line">  gradient = [...code to compute derivative of J(theta)...];</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>MATLAB提供了<code>fminunc()</code>函数来实现优化：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">100</span>);</div><div class="line">initialTheta = <span class="built_in">zeros</span>(<span class="number">2</span>,<span class="number">1</span>);</div><div class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</div></pre></td></tr></table></figure><h2 id="过拟合-Overfitting-和正则化-Regularization"><a href="#过拟合-Overfitting-和正则化-Regularization" class="headerlink" title="过拟合(Overfitting)和正则化(Regularization)"></a>过拟合(Overfitting)和正则化(Regularization)</h2><p><code>欠拟合(underfitting)</code>或<code>高偏差(high bias)</code>，是指假设函数不足以描述数据趋势，通常由假设函数过于简单或者特征维数太少导致；<br><code>过拟合(overfitting)</code>或<code>高方差(high variance)</code>，是指假设函数对训练集拟合良好但对测试集的通用性差，通常由于假设函数过于复杂或特征维数太高。</p><p>解决过拟合问题的常用方法：</p><ol><li>减少特征维数：手动选择特征、使用模型选择算法；</li><li><code>正则化(Regularization)</code>：保留所有特征，但减小参数\(\theta_j\)的量级；正则化在有许多弱相关特征(slightly useful features)的时候很有用。</li></ol><h3 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h3><h4 id="损失函数正则化"><a href="#损失函数正则化" class="headerlink" title="损失函数正则化"></a>损失函数正则化</h4><p>正则化其实就是通过增加参数在损失函数中的影响来减弱其最后在假设函数中的权重：<br>例如，对于假设函数\(\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4\)，如果想让它变得更加“二次方”，即排除\(\theta_3x^3 \)和\(\theta_4x^4\)的影响，可以通过修改损失函数来实现：</p><p>$$min_\theta \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2$$</p><p>正则化所有参数（除了\(\theta_0\)）：</p><p>$$min_\theta \dfrac{1}{2m}  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n \theta_j^2$$</p><p>其中\(\lambda\)是<code>正则化系数(regularization parameter)</code>，\(\lambda\)越大，拟合曲线越平滑，但要注意的是\(\lambda\)过大可能导致欠拟合。</p><h4 id="梯度下降正则化"><a href="#梯度下降正则化" class="headerlink" title="梯度下降正则化"></a>梯度下降正则化</h4><p>对于线性回归而言，正则化后的梯度下降：</p><p>$$\begin{align} &amp; \text{Repeat} \lbrace \newline &amp;     \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;     \theta_j := \theta_j - \alpha \left[ \left( \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;          j \in \lbrace 1,2…n\rbrace\newline &amp; \rbrace \end{align}$$ </p><h4 id="法方程正则化"><a href="#法方程正则化" class="headerlink" title="法方程正则化"></a>法方程正则化</h4><p>正则化后的法方程可以写为：</p><p>$$\begin{align}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}  L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align}$$</p><p>要注意梯度下降中的下标\(j\)是从1开始的，同样，法方程中\(\lambda(1,1)=0\)，这是因为截取项\(\theta_0\)是不需要正则化的。</p><h3 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h3><h4 id="正则化后的损失函数"><a href="#正则化后的损失函数" class="headerlink" title="正则化后的损失函数"></a>正则化后的损失函数</h4><p>逻辑回归的损失函数加上正则化项：</p><p>$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)} \log (h_\theta (x^{(i)})) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$</p><p>注意\(\sum_{j=1}^n \theta_j^2\)下标从1开始，在梯度下降中也要特别注意\(\theta_0\)的更新。</p><h4 id="正则化后的梯度下降"><a href="#正则化后的梯度下降" class="headerlink" title="正则化后的梯度下降"></a>正则化后的梯度下降</h4><p>$$\begin{align} &amp; \text{Repeat} \lbrace \newline &amp;     \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;     \theta_j := \theta_j - \alpha \left[ \left( \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;          j \in \lbrace 1,2…n\rbrace\newline &amp; \rbrace \end{align}$$ </p><h2 id="多值分类"><a href="#多值分类" class="headerlink" title="多值分类"></a>多值分类</h2><p>当预测分类值有多个，如\(y={0,1,2,…,n}\)时，可以将其看做\(n+1\)个二值分类问题：对于每一类，将其看做一类而其他所有的看做一类。</p><p>$$\begin{align}&amp; y \in \lbrace0, 1 … n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align}$$</p><ol><li>训练时，对每一类做二值逻辑回归分类器训练；</li><li>预测时，计算每类的预测值，最大的为结果。</li></ol><p>举个例子（3值分类）：</p><img src="/2017/06/03/logisticreg/3classes.png">]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;分类表达式&quot;&gt;&lt;a href=&quot;#分类表达式&quot; class=&quot;headerlink&quot; title=&quot;分类表达式&quot;&gt;&lt;/a&gt;分类表达式&lt;/h2&gt;&lt;h3 id=&quot;分类和回归&quot;&gt;&lt;a href=&quot;#分类和回归&quot; class=&quot;headerlink&quot; title=&quot;分类和回归&quot;&gt;&lt;/a&gt;分类和回归&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;分类问题和回归问题的形式是相似的，但它的预测值是一组有限的离散值。  &lt;/li&gt;
&lt;li&gt;分类问题的求解可以看做是找边界线（面）的过程，即拟合边界曲线（面），特征空间内的点分布在该曲线（面）分割出来的不同子空间中；  &lt;/li&gt;
&lt;li&gt;而回归问题的求解是拟合特征-预测值曲线（面），特征空间内的点分布在该曲线（面）上或者附近。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;分类问题的假设表达式&quot;&gt;&lt;a href=&quot;#分类问题的假设表达式&quot; class=&quot;headerlink&quot; title=&quot;分类问题的假设表达式&quot;&gt;&lt;/a&gt;分类问题的假设表达式&lt;/h3&gt;&lt;h4 id=&quot;逻辑函数-Logistic-Function-or-Sigmoid-Function&quot;&gt;&lt;a href=&quot;#逻辑函数-Logistic-Function-or-Sigmoid-Function&quot; class=&quot;headerlink&quot; title=&quot;逻辑函数(Logistic Function or Sigmoid Function)&quot;&gt;&lt;/a&gt;逻辑函数(Logistic Function or Sigmoid Function)&lt;/h4&gt;&lt;p&gt;使用逻辑函数\(g(z)\)将预测值映射到[0, 1]区间中，使假设函数更适用于分类问题。&lt;/p&gt;
&lt;p&gt;$$\begin{align}&amp;amp;&lt;br&gt;h_\theta (x) = g ( \theta^T x )&lt;br&gt;\newline&lt;br&gt;\newline&amp;amp; z = \theta^T x&lt;br&gt;\newline&amp;amp; g(z) = \dfrac{1}{1 + e^{-z}}&lt;br&gt;\end{align}$$&lt;/p&gt;
    
    </summary>
    
      <category term="NG_ML" scheme="http://huisblog.cn/categories/NG-ML/"/>
    
    
      <category term="ML" scheme="http://huisblog.cn/tags/ML/"/>
    
  </entry>
  
</feed>
